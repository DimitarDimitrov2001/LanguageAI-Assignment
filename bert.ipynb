{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Preprocessing:\n",
    "Clean and preprocess your text data as needed.\n",
    "Tokenize the text into words or subwords.\n",
    "\n",
    "2. BERT Embeddings:\n",
    "Use Hugging Face's Transformers library to load a pre-trained BERT model.\n",
    "Obtain contextualized word embeddings for each word in your text data.\n",
    "\n",
    "3.Topic Modeling:\n",
    "Apply a topic modeling technique (e.g., LDA or NMF) to extract latent topics from the preprocessed text data.\n",
    "\n",
    "4. Feature Extraction:\n",
    "Combine the BERT embeddings and topic distributions for each document. You can concatenate or merge these features.\n",
    "\n",
    "5. Model Training:\n",
    "Choose a classification model (e.g., logistic regression, random forest, or a neural network) for each dichotomy.\n",
    "Train each model on the combined feature matrix, using the corresponding labels.\n",
    "\n",
    "6. Evaluation:\n",
    "Evaluate the performance of each model on a testing set using appropriate metrics.\n",
    "\n",
    "7. Hyperparameter Tuning:\n",
    "Fine-tune the hyperparameters of each model to optimize performance.\n",
    "\n",
    "8. Interpretation:\n",
    "Analyze the importance of different features, including BERT embeddings and topic distributions, for each dichotomy to gain insights into how they contribute to personality type prediction.\n",
    "By building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned = pd.read_csv(r'C:\\Users\\bella\\Downloads\\Y3Q2 langai\\df_all_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auhtor_ID</th>\n",
       "      <th>post_extrovert</th>\n",
       "      <th>extrovert</th>\n",
       "      <th>post_feeling</th>\n",
       "      <th>feeling</th>\n",
       "      <th>post_judging</th>\n",
       "      <th>judging</th>\n",
       "      <th>post_sensing</th>\n",
       "      <th>sensing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_12bhu7</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_12jbpd</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_12uwr5</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_12zm15</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_13cjjl</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>t2_vfp8y</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>t2_w0842</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>t2_w6rgl</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>t2_wilcwvo</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t2_zq7gkv</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auhtor_ID                                     post_extrovert  extrovert  \\\n",
       "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
       "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
       "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
       "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
       "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
       "..          ...                                                ...        ...   \n",
       "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
       "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
       "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
       "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
       "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
       "\n",
       "                                          post_feeling  feeling  \\\n",
       "0    I wear a Lorna shore shirt out alot in public ...      1.0   \n",
       "1    I'd say this is a very accurate characterizati...      0.0   \n",
       "2    Ya know like most people with home decorations...      0.0   \n",
       "3    It's true tho. They're kinda more interesting ...      1.0   \n",
       "4    Yeah, but that's one of the things that make m...      0.0   \n",
       "..                                                 ...      ...   \n",
       "150  so change profession then. this would be inadm...      0.0   \n",
       "151  The technological singularity. And the possibi...      0.0   \n",
       "152  Dear God man. Chill. I'm not Einstein or Hawki...      0.0   \n",
       "153  That's what a fake lib would say [Human blood ...      0.0   \n",
       "154  My biggest problem is asking for it. I don’t n...      1.0   \n",
       "\n",
       "                                          post_judging  judging  \\\n",
       "0    I wear a Lorna shore shirt out alot in public ...      0.0   \n",
       "1    I'd say this is a very accurate characterizati...      0.0   \n",
       "2    Ya know like most people with home decorations...      1.0   \n",
       "3    It's true tho. They're kinda more interesting ...      0.0   \n",
       "4    Yeah, but that's one of the things that make m...      0.0   \n",
       "..                                                 ...      ...   \n",
       "150  so change profession then. this would be inadm...      1.0   \n",
       "151  The technological singularity. And the possibi...      1.0   \n",
       "152  Dear God man. Chill. I'm not Einstein or Hawki...      1.0   \n",
       "153  That's what a fake lib would say [Human blood ...      0.0   \n",
       "154  My biggest problem is asking for it. I don’t n...      1.0   \n",
       "\n",
       "                                          post_sensing  sensing  \n",
       "0    I wear a Lorna shore shirt out alot in public ...      0.0  \n",
       "1    I'd say this is a very accurate characterizati...      0.0  \n",
       "2    Ya know like most people with home decorations...      0.0  \n",
       "3    It's true tho. They're kinda more interesting ...      0.0  \n",
       "4    Yeah, but that's one of the things that make m...      1.0  \n",
       "..                                                 ...      ...  \n",
       "150  so change profession then. this would be inadm...      0.0  \n",
       "151  The technological singularity. And the possibi...      0.0  \n",
       "152  Dear God man. Chill. I'm not Einstein or Hawki...      0.0  \n",
       "153  That's what a fake lib would say [Human blood ...      0.0  \n",
       "154  My biggest problem is asking for it. I don’t n...      0.0  \n",
       "\n",
       "[155 rows x 9 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_all_cleaned = df_all_cleaned.drop(['post_feeling', 'post_judging', 'post_sensing'], axis=1)\n",
    "df_all_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bella\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bella\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auhtor_ID</th>\n",
       "      <th>post_extrovert</th>\n",
       "      <th>extrovert</th>\n",
       "      <th>post_feeling</th>\n",
       "      <th>feeling</th>\n",
       "      <th>post_judging</th>\n",
       "      <th>judging</th>\n",
       "      <th>post_sensing</th>\n",
       "      <th>sensing</th>\n",
       "      <th>processed_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_12bhu7</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wear lorna shore shirt alot public lewd long s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_12jbpd</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>id say accurate characterization ni users read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_12uwr5</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ya know like people home decorations could sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_12zm15</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>true tho theyre kinda interesting buuuut issue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_13cjjl</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yeah thats one things make better objectively ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>t2_vfp8y</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>change profession would inadmissible country p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>t2_w0842</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technological singularity possibility contribu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>t2_w6rgl</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dear god man chill im einstein hawking serious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>t2_wilcwvo</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thats fake lib would say human blood water url...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t2_zq7gkv</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>biggest problem asking dont need amount recipr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auhtor_ID                                     post_extrovert  extrovert  \\\n",
       "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
       "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
       "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
       "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
       "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
       "..          ...                                                ...        ...   \n",
       "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
       "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
       "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
       "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
       "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
       "\n",
       "                                          post_feeling  feeling  \\\n",
       "0    I wear a Lorna shore shirt out alot in public ...      1.0   \n",
       "1    I'd say this is a very accurate characterizati...      0.0   \n",
       "2    Ya know like most people with home decorations...      0.0   \n",
       "3    It's true tho. They're kinda more interesting ...      1.0   \n",
       "4    Yeah, but that's one of the things that make m...      0.0   \n",
       "..                                                 ...      ...   \n",
       "150  so change profession then. this would be inadm...      0.0   \n",
       "151  The technological singularity. And the possibi...      0.0   \n",
       "152  Dear God man. Chill. I'm not Einstein or Hawki...      0.0   \n",
       "153  That's what a fake lib would say [Human blood ...      0.0   \n",
       "154  My biggest problem is asking for it. I don’t n...      1.0   \n",
       "\n",
       "                                          post_judging  judging  \\\n",
       "0    I wear a Lorna shore shirt out alot in public ...      0.0   \n",
       "1    I'd say this is a very accurate characterizati...      0.0   \n",
       "2    Ya know like most people with home decorations...      1.0   \n",
       "3    It's true tho. They're kinda more interesting ...      0.0   \n",
       "4    Yeah, but that's one of the things that make m...      0.0   \n",
       "..                                                 ...      ...   \n",
       "150  so change profession then. this would be inadm...      1.0   \n",
       "151  The technological singularity. And the possibi...      1.0   \n",
       "152  Dear God man. Chill. I'm not Einstein or Hawki...      1.0   \n",
       "153  That's what a fake lib would say [Human blood ...      0.0   \n",
       "154  My biggest problem is asking for it. I don’t n...      1.0   \n",
       "\n",
       "                                          post_sensing  sensing  \\\n",
       "0    I wear a Lorna shore shirt out alot in public ...      0.0   \n",
       "1    I'd say this is a very accurate characterizati...      0.0   \n",
       "2    Ya know like most people with home decorations...      0.0   \n",
       "3    It's true tho. They're kinda more interesting ...      0.0   \n",
       "4    Yeah, but that's one of the things that make m...      1.0   \n",
       "..                                                 ...      ...   \n",
       "150  so change profession then. this would be inadm...      0.0   \n",
       "151  The technological singularity. And the possibi...      0.0   \n",
       "152  Dear God man. Chill. I'm not Einstein or Hawki...      0.0   \n",
       "153  That's what a fake lib would say [Human blood ...      0.0   \n",
       "154  My biggest problem is asking for it. I don’t n...      0.0   \n",
       "\n",
       "                                        processed_post  \n",
       "0    wear lorna shore shirt alot public lewd long s...  \n",
       "1    id say accurate characterization ni users read...  \n",
       "2    ya know like people home decorations could sav...  \n",
       "3    true tho theyre kinda interesting buuuut issue...  \n",
       "4    yeah thats one things make better objectively ...  \n",
       "..                                                 ...  \n",
       "150  change profession would inadmissible country p...  \n",
       "151  technological singularity possibility contribu...  \n",
       "152  dear god man chill im einstein hawking serious...  \n",
       "153  thats fake lib would say human blood water url...  \n",
       "154  biggest problem asking dont need amount recipr...  \n",
       "\n",
       "[155 rows x 10 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_cleaned['processed_post'] = df_all_cleaned['post_extrovert'].apply(preprocess_text)\n",
    "df_all_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      auhtor_ID                                     post_extrovert  extrovert  \\\n",
      "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
      "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
      "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
      "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
      "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
      "..          ...                                                ...        ...   \n",
      "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
      "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
      "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
      "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
      "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
      "\n",
      "                                          post_feeling  feeling  \\\n",
      "0    I wear a Lorna shore shirt out alot in public ...      1.0   \n",
      "1    I'd say this is a very accurate characterizati...      0.0   \n",
      "2    Ya know like most people with home decorations...      0.0   \n",
      "3    It's true tho. They're kinda more interesting ...      1.0   \n",
      "4    Yeah, but that's one of the things that make m...      0.0   \n",
      "..                                                 ...      ...   \n",
      "150  so change profession then. this would be inadm...      0.0   \n",
      "151  The technological singularity. And the possibi...      0.0   \n",
      "152  Dear God man. Chill. I'm not Einstein or Hawki...      0.0   \n",
      "153  That's what a fake lib would say [Human blood ...      0.0   \n",
      "154  My biggest problem is asking for it. I don’t n...      1.0   \n",
      "\n",
      "                                          post_judging  judging  \\\n",
      "0    I wear a Lorna shore shirt out alot in public ...      0.0   \n",
      "1    I'd say this is a very accurate characterizati...      0.0   \n",
      "2    Ya know like most people with home decorations...      1.0   \n",
      "3    It's true tho. They're kinda more interesting ...      0.0   \n",
      "4    Yeah, but that's one of the things that make m...      0.0   \n",
      "..                                                 ...      ...   \n",
      "150  so change profession then. this would be inadm...      1.0   \n",
      "151  The technological singularity. And the possibi...      1.0   \n",
      "152  Dear God man. Chill. I'm not Einstein or Hawki...      1.0   \n",
      "153  That's what a fake lib would say [Human blood ...      0.0   \n",
      "154  My biggest problem is asking for it. I don’t n...      1.0   \n",
      "\n",
      "                                          post_sensing  sensing  \\\n",
      "0    I wear a Lorna shore shirt out alot in public ...      0.0   \n",
      "1    I'd say this is a very accurate characterizati...      0.0   \n",
      "2    Ya know like most people with home decorations...      0.0   \n",
      "3    It's true tho. They're kinda more interesting ...      0.0   \n",
      "4    Yeah, but that's one of the things that make m...      1.0   \n",
      "..                                                 ...      ...   \n",
      "150  so change profession then. this would be inadm...      0.0   \n",
      "151  The technological singularity. And the possibi...      0.0   \n",
      "152  Dear God man. Chill. I'm not Einstein or Hawki...      0.0   \n",
      "153  That's what a fake lib would say [Human blood ...      0.0   \n",
      "154  My biggest problem is asking for it. I don’t n...      0.0   \n",
      "\n",
      "                                        processed_post  \\\n",
      "0    wear lorna shore shirt alot public lewd long s...   \n",
      "1    id say accurate characterization ni users read...   \n",
      "2    ya know like people home decorations could sav...   \n",
      "3    true tho theyre kinda interesting buuuut issue...   \n",
      "4    yeah thats one things make better objectively ...   \n",
      "..                                                 ...   \n",
      "150  change profession would inadmissible country p...   \n",
      "151  technological singularity possibility contribu...   \n",
      "152  dear god man chill im einstein hawking serious...   \n",
      "153  thats fake lib would say human blood water url...   \n",
      "154  biggest problem asking dont need amount recipr...   \n",
      "\n",
      "                                       bert_embeddings  \n",
      "0    [0.03740044, 0.03744348, 0.40402788, -0.154586...  \n",
      "1    [-0.122634806, 0.06978733, 0.23516777, -0.1674...  \n",
      "2    [0.10358047, -0.079817355, 0.4862674, 0.006832...  \n",
      "3    [-0.11131706, 0.070213296, 0.5168624, 0.017204...  \n",
      "4    [0.21926472, 0.11031033, 0.28619415, 0.1073027...  \n",
      "..                                                 ...  \n",
      "150  [-0.17524596, 0.1841626, 0.44777465, -0.118973...  \n",
      "151  [-0.025105778, -0.08084041, 0.3463775, -0.0189...  \n",
      "152  [0.088415004, 0.22571912, 0.38455746, -0.03194...  \n",
      "153  [0.05957357, 0.10176626, 0.3839616, -0.0742901...  \n",
      "154  [-0.06220896, 0.07866823, 0.22912164, 0.002138...  \n",
      "\n",
      "[155 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to obtain BERT embeddings for a text\n",
    "def get_bert_embeddings(text):\n",
    "    # Tokenize input text and convert to tensor\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "    # Get BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(tokens)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "\n",
    "    # Average the embeddings across tokens (you can modify this based on your needs)\n",
    "    avg_embedding = torch.mean(embeddings, dim=1).squeeze().numpy()\n",
    "\n",
    "    return avg_embedding\n",
    "\n",
    "# # Example DataFrame with a 'processed_post' column\n",
    "# data = {'processed_post': [\"enjoy hiking spending time nature\",\n",
    "#                             \"text data preprocessing crucial nlp tasks\",\n",
    "#                             \"stop words removal improves text analysis\"]}\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Apply BERT embeddings to the 'processed_post' column\n",
    "# df['bert_embeddings'] = df['processed_post'].apply(get_bert_embeddings)\n",
    "\n",
    "# # Display the DataFrame with processed text and BERT embeddings\n",
    "# print(df)\n",
    "\n",
    "df_all_cleaned['bert_embeddings'] = df_all_cleaned['post_extrovert'].apply(get_bert_embeddings)\n",
    "print(df_all_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auhtor_ID</th>\n",
       "      <th>post_extrovert</th>\n",
       "      <th>extrovert</th>\n",
       "      <th>post_feeling</th>\n",
       "      <th>feeling</th>\n",
       "      <th>post_judging</th>\n",
       "      <th>judging</th>\n",
       "      <th>post_sensing</th>\n",
       "      <th>sensing</th>\n",
       "      <th>processed_post</th>\n",
       "      <th>bert_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_12bhu7</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wear lorna shore shirt alot public lewd long s...</td>\n",
       "      <td>[0.03740044, 0.03744348, 0.40402788, -0.154586...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_12jbpd</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>id say accurate characterization ni users read...</td>\n",
       "      <td>[-0.122634806, 0.06978733, 0.23516777, -0.1674...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_12uwr5</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ya know like people home decorations could sav...</td>\n",
       "      <td>[0.10358047, -0.079817355, 0.4862674, 0.006832...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_12zm15</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>true tho theyre kinda interesting buuuut issue...</td>\n",
       "      <td>[-0.11131706, 0.070213296, 0.5168624, 0.017204...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_13cjjl</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yeah thats one things make better objectively ...</td>\n",
       "      <td>[0.21926472, 0.11031033, 0.28619415, 0.1073027...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>t2_vfp8y</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>change profession would inadmissible country p...</td>\n",
       "      <td>[-0.17524596, 0.1841626, 0.44777465, -0.118973...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>t2_w0842</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technological singularity possibility contribu...</td>\n",
       "      <td>[-0.025105778, -0.08084041, 0.3463775, -0.0189...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>t2_w6rgl</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dear god man chill im einstein hawking serious...</td>\n",
       "      <td>[0.088415004, 0.22571912, 0.38455746, -0.03194...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>t2_wilcwvo</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thats fake lib would say human blood water url...</td>\n",
       "      <td>[0.05957357, 0.10176626, 0.3839616, -0.0742901...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t2_zq7gkv</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>biggest problem asking dont need amount recipr...</td>\n",
       "      <td>[-0.06220896, 0.07866823, 0.22912164, 0.002138...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auhtor_ID                                     post_extrovert  extrovert  \\\n",
       "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
       "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
       "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
       "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
       "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
       "..          ...                                                ...        ...   \n",
       "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
       "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
       "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
       "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
       "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
       "\n",
       "                                          post_feeling  feeling  \\\n",
       "0    I wear a Lorna shore shirt out alot in public ...      1.0   \n",
       "1    I'd say this is a very accurate characterizati...      0.0   \n",
       "2    Ya know like most people with home decorations...      0.0   \n",
       "3    It's true tho. They're kinda more interesting ...      1.0   \n",
       "4    Yeah, but that's one of the things that make m...      0.0   \n",
       "..                                                 ...      ...   \n",
       "150  so change profession then. this would be inadm...      0.0   \n",
       "151  The technological singularity. And the possibi...      0.0   \n",
       "152  Dear God man. Chill. I'm not Einstein or Hawki...      0.0   \n",
       "153  That's what a fake lib would say [Human blood ...      0.0   \n",
       "154  My biggest problem is asking for it. I don’t n...      1.0   \n",
       "\n",
       "                                          post_judging  judging  \\\n",
       "0    I wear a Lorna shore shirt out alot in public ...      0.0   \n",
       "1    I'd say this is a very accurate characterizati...      0.0   \n",
       "2    Ya know like most people with home decorations...      1.0   \n",
       "3    It's true tho. They're kinda more interesting ...      0.0   \n",
       "4    Yeah, but that's one of the things that make m...      0.0   \n",
       "..                                                 ...      ...   \n",
       "150  so change profession then. this would be inadm...      1.0   \n",
       "151  The technological singularity. And the possibi...      1.0   \n",
       "152  Dear God man. Chill. I'm not Einstein or Hawki...      1.0   \n",
       "153  That's what a fake lib would say [Human blood ...      0.0   \n",
       "154  My biggest problem is asking for it. I don’t n...      1.0   \n",
       "\n",
       "                                          post_sensing  sensing  \\\n",
       "0    I wear a Lorna shore shirt out alot in public ...      0.0   \n",
       "1    I'd say this is a very accurate characterizati...      0.0   \n",
       "2    Ya know like most people with home decorations...      0.0   \n",
       "3    It's true tho. They're kinda more interesting ...      0.0   \n",
       "4    Yeah, but that's one of the things that make m...      1.0   \n",
       "..                                                 ...      ...   \n",
       "150  so change profession then. this would be inadm...      0.0   \n",
       "151  The technological singularity. And the possibi...      0.0   \n",
       "152  Dear God man. Chill. I'm not Einstein or Hawki...      0.0   \n",
       "153  That's what a fake lib would say [Human blood ...      0.0   \n",
       "154  My biggest problem is asking for it. I don’t n...      0.0   \n",
       "\n",
       "                                        processed_post  \\\n",
       "0    wear lorna shore shirt alot public lewd long s...   \n",
       "1    id say accurate characterization ni users read...   \n",
       "2    ya know like people home decorations could sav...   \n",
       "3    true tho theyre kinda interesting buuuut issue...   \n",
       "4    yeah thats one things make better objectively ...   \n",
       "..                                                 ...   \n",
       "150  change profession would inadmissible country p...   \n",
       "151  technological singularity possibility contribu...   \n",
       "152  dear god man chill im einstein hawking serious...   \n",
       "153  thats fake lib would say human blood water url...   \n",
       "154  biggest problem asking dont need amount recipr...   \n",
       "\n",
       "                                       bert_embeddings  \n",
       "0    [0.03740044, 0.03744348, 0.40402788, -0.154586...  \n",
       "1    [-0.122634806, 0.06978733, 0.23516777, -0.1674...  \n",
       "2    [0.10358047, -0.079817355, 0.4862674, 0.006832...  \n",
       "3    [-0.11131706, 0.070213296, 0.5168624, 0.017204...  \n",
       "4    [0.21926472, 0.11031033, 0.28619415, 0.1073027...  \n",
       "..                                                 ...  \n",
       "150  [-0.17524596, 0.1841626, 0.44777465, -0.118973...  \n",
       "151  [-0.025105778, -0.08084041, 0.3463775, -0.0189...  \n",
       "152  [0.088415004, 0.22571912, 0.38455746, -0.03194...  \n",
       "153  [0.05957357, 0.10176626, 0.3839616, -0.0742901...  \n",
       "154  [-0.06220896, 0.07866823, 0.22912164, 0.002138...  \n",
       "\n",
       "[155 rows x 11 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Function to apply LDA and return topic distributions\n",
    "# def get_topic_distribution(text):\n",
    "#     vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "#     text_vectorized = vectorizer.fit_transform([text])\n",
    "    \n",
    "#     lda = LatentDirichletAllocation(n_components=5, random_state=42)  # Adjust the number of topics as needed\n",
    "#     topic_distribution = lda.fit_transform(text_vectorized)\n",
    "    \n",
    "#     return topic_distribution.flatten()\n",
    "\n",
    "# # Assume 'processed_post' is the column with processed text\n",
    "# df_all_cleaned['topic_distribution'] = df_all_cleaned['processed_post'].apply(get_topic_distribution)\n",
    "\n",
    "# # Display the DataFrame with combined features\n",
    "# print(df_all_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine BERT embeddings and topic distributions\n",
    "# # df_all_cleaned['combined_features'] = df_all_cleaned.apply(lambda row: np.concatenate([row['bert_embeddings'], row['topic_distribution']]), axis=1)\n",
    "\n",
    "# df_all_cleaned['combined_features'] = df_all_cleaned.apply(lambda row: 0.5 * (row['bert_embeddings'] + row['topic_distribution']), axis=1)\n",
    "\n",
    "\n",
    "# # Display the DataFrame with combined features\n",
    "# print(df_all_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auhtor_ID</th>\n",
       "      <th>post_extrovert</th>\n",
       "      <th>extrovert</th>\n",
       "      <th>post_feeling</th>\n",
       "      <th>feeling</th>\n",
       "      <th>post_judging</th>\n",
       "      <th>judging</th>\n",
       "      <th>post_sensing</th>\n",
       "      <th>sensing</th>\n",
       "      <th>processed_post</th>\n",
       "      <th>bert_embeddings</th>\n",
       "      <th>topic_distribution</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_12bhu7</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wear lorna shore shirt alot public lewd long s...</td>\n",
       "      <td>[0.03740044, 0.03744348, 0.40402788, -0.154586...</td>\n",
       "      <td>[0.9997959152018512, 5.102119953720001e-05, 5....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_12jbpd</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>id say accurate characterization ni users read...</td>\n",
       "      <td>[-0.122634806, 0.06978733, 0.23516777, -0.1674...</td>\n",
       "      <td>[1.941758505806631e-05, 1.9417585058066305e-05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_12uwr5</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ya know like people home decorations could sav...</td>\n",
       "      <td>[0.10358047, -0.079817355, 0.4862674, 0.006832...</td>\n",
       "      <td>[0.9999582903714797, 1.0427407129993352e-05, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_12zm15</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>true tho theyre kinda interesting buuuut issue...</td>\n",
       "      <td>[-0.11131706, 0.070213296, 0.5168624, 0.017204...</td>\n",
       "      <td>[0.9999588540280505, 1.0286492987415484e-05, 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_13cjjl</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yeah thats one things make better objectively ...</td>\n",
       "      <td>[0.21926472, 0.11031033, 0.28619415, 0.1073027...</td>\n",
       "      <td>[0.9997812483957159, 5.4687901070990636e-05, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>t2_vfp8y</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>change profession would inadmissible country p...</td>\n",
       "      <td>[-0.17524596, 0.1841626, 0.44777465, -0.118973...</td>\n",
       "      <td>[1.265631868878355e-05, 1.2656318688783549e-05...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>t2_w0842</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technological singularity possibility contribu...</td>\n",
       "      <td>[-0.025105778, -0.08084041, 0.3463775, -0.0189...</td>\n",
       "      <td>[2.957185171821599e-05, 2.957185171821599e-05,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>t2_w6rgl</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dear god man chill im einstein hawking serious...</td>\n",
       "      <td>[0.088415004, 0.22571912, 0.38455746, -0.03194...</td>\n",
       "      <td>[0.9999604284773709, 9.892880657306835e-06, 9....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>t2_wilcwvo</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thats fake lib would say human blood water url...</td>\n",
       "      <td>[0.05957357, 0.10176626, 0.3839616, -0.0742901...</td>\n",
       "      <td>[0.9998581033172271, 3.547417069318599e-05, 3....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t2_zq7gkv</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>biggest problem asking dont need amount recipr...</td>\n",
       "      <td>[-0.06220896, 0.07866823, 0.22912164, 0.002138...</td>\n",
       "      <td>[0.9999273346780284, 1.8166330492961654e-05, 1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auhtor_ID                                     post_extrovert  extrovert  \\\n",
       "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
       "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
       "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
       "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
       "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
       "..          ...                                                ...        ...   \n",
       "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
       "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
       "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
       "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
       "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
       "\n",
       "                                          post_feeling  feeling  \\\n",
       "0    I wear a Lorna shore shirt out alot in public ...      1.0   \n",
       "1    I'd say this is a very accurate characterizati...      0.0   \n",
       "2    Ya know like most people with home decorations...      0.0   \n",
       "3    It's true tho. They're kinda more interesting ...      1.0   \n",
       "4    Yeah, but that's one of the things that make m...      0.0   \n",
       "..                                                 ...      ...   \n",
       "150  so change profession then. this would be inadm...      0.0   \n",
       "151  The technological singularity. And the possibi...      0.0   \n",
       "152  Dear God man. Chill. I'm not Einstein or Hawki...      0.0   \n",
       "153  That's what a fake lib would say [Human blood ...      0.0   \n",
       "154  My biggest problem is asking for it. I don’t n...      1.0   \n",
       "\n",
       "                                          post_judging  judging  \\\n",
       "0    I wear a Lorna shore shirt out alot in public ...      0.0   \n",
       "1    I'd say this is a very accurate characterizati...      0.0   \n",
       "2    Ya know like most people with home decorations...      1.0   \n",
       "3    It's true tho. They're kinda more interesting ...      0.0   \n",
       "4    Yeah, but that's one of the things that make m...      0.0   \n",
       "..                                                 ...      ...   \n",
       "150  so change profession then. this would be inadm...      1.0   \n",
       "151  The technological singularity. And the possibi...      1.0   \n",
       "152  Dear God man. Chill. I'm not Einstein or Hawki...      1.0   \n",
       "153  That's what a fake lib would say [Human blood ...      0.0   \n",
       "154  My biggest problem is asking for it. I don’t n...      1.0   \n",
       "\n",
       "                                          post_sensing  sensing  \\\n",
       "0    I wear a Lorna shore shirt out alot in public ...      0.0   \n",
       "1    I'd say this is a very accurate characterizati...      0.0   \n",
       "2    Ya know like most people with home decorations...      0.0   \n",
       "3    It's true tho. They're kinda more interesting ...      0.0   \n",
       "4    Yeah, but that's one of the things that make m...      1.0   \n",
       "..                                                 ...      ...   \n",
       "150  so change profession then. this would be inadm...      0.0   \n",
       "151  The technological singularity. And the possibi...      0.0   \n",
       "152  Dear God man. Chill. I'm not Einstein or Hawki...      0.0   \n",
       "153  That's what a fake lib would say [Human blood ...      0.0   \n",
       "154  My biggest problem is asking for it. I don’t n...      0.0   \n",
       "\n",
       "                                        processed_post  \\\n",
       "0    wear lorna shore shirt alot public lewd long s...   \n",
       "1    id say accurate characterization ni users read...   \n",
       "2    ya know like people home decorations could sav...   \n",
       "3    true tho theyre kinda interesting buuuut issue...   \n",
       "4    yeah thats one things make better objectively ...   \n",
       "..                                                 ...   \n",
       "150  change profession would inadmissible country p...   \n",
       "151  technological singularity possibility contribu...   \n",
       "152  dear god man chill im einstein hawking serious...   \n",
       "153  thats fake lib would say human blood water url...   \n",
       "154  biggest problem asking dont need amount recipr...   \n",
       "\n",
       "                                       bert_embeddings  \\\n",
       "0    [0.03740044, 0.03744348, 0.40402788, -0.154586...   \n",
       "1    [-0.122634806, 0.06978733, 0.23516777, -0.1674...   \n",
       "2    [0.10358047, -0.079817355, 0.4862674, 0.006832...   \n",
       "3    [-0.11131706, 0.070213296, 0.5168624, 0.017204...   \n",
       "4    [0.21926472, 0.11031033, 0.28619415, 0.1073027...   \n",
       "..                                                 ...   \n",
       "150  [-0.17524596, 0.1841626, 0.44777465, -0.118973...   \n",
       "151  [-0.025105778, -0.08084041, 0.3463775, -0.0189...   \n",
       "152  [0.088415004, 0.22571912, 0.38455746, -0.03194...   \n",
       "153  [0.05957357, 0.10176626, 0.3839616, -0.0742901...   \n",
       "154  [-0.06220896, 0.07866823, 0.22912164, 0.002138...   \n",
       "\n",
       "                                    topic_distribution  \n",
       "0    [0.9997959152018512, 5.102119953720001e-05, 5....  \n",
       "1    [1.941758505806631e-05, 1.9417585058066305e-05...  \n",
       "2    [0.9999582903714797, 1.0427407129993352e-05, 1...  \n",
       "3    [0.9999588540280505, 1.0286492987415484e-05, 1...  \n",
       "4    [0.9997812483957159, 5.4687901070990636e-05, 5...  \n",
       "..                                                 ...  \n",
       "150  [1.265631868878355e-05, 1.2656318688783549e-05...  \n",
       "151  [2.957185171821599e-05, 2.957185171821599e-05,...  \n",
       "152  [0.9999604284773709, 9.892880657306835e-06, 9....  \n",
       "153  [0.9998581033172271, 3.547417069318599e-05, 3....  \n",
       "154  [0.9999273346780284, 1.8166330492961654e-05, 1...  \n",
       "\n",
       "[155 rows x 12 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 1.99 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extrovert</th>\n",
       "      <th>feeling</th>\n",
       "      <th>judging</th>\n",
       "      <th>sensing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     extrovert  feeling  judging  sensing\n",
       "0          1.0      1.0      0.0      0.0\n",
       "1          1.0      0.0      0.0      0.0\n",
       "2          0.0      0.0      1.0      0.0\n",
       "3          0.0      1.0      0.0      0.0\n",
       "4          0.0      0.0      0.0      1.0\n",
       "..         ...      ...      ...      ...\n",
       "150        0.0      0.0      1.0      0.0\n",
       "151        0.0      0.0      1.0      0.0\n",
       "152        0.0      0.0      1.0      0.0\n",
       "153        1.0      0.0      0.0      0.0\n",
       "154        1.0      1.0      1.0      0.0\n",
       "\n",
       "[155 rows x 4 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_labels =  pd.DataFrame()\n",
    "df_labels['extrovert']= df_all_cleaned['extrovert']\n",
    "df_labels['feeling']= df_all_cleaned['feeling']\n",
    "df_labels['judging']= df_all_cleaned['judging']\n",
    "df_labels['sensing']= df_all_cleaned['sensing']\n",
    "\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 58ms/step - loss: 0.6595 - accuracy: 0.2581 - val_loss: 0.5479 - val_accuracy: 0.3226\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.5527 - accuracy: 0.3710 - val_loss: 0.5740 - val_accuracy: 0.3226\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5540 - accuracy: 0.3710 - val_loss: 0.5774 - val_accuracy: 0.3226\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.5451 - accuracy: 0.3710 - val_loss: 0.5615 - val_accuracy: 0.3226\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.5394 - accuracy: 0.3710 - val_loss: 0.5435 - val_accuracy: 0.3226\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5290 - accuracy: 0.3710 - val_loss: 0.5434 - val_accuracy: 0.3226\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5223 - accuracy: 0.3710 - val_loss: 0.5474 - val_accuracy: 0.3226\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5163 - accuracy: 0.3710 - val_loss: 0.5474 - val_accuracy: 0.3226\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5108 - accuracy: 0.3710 - val_loss: 0.5401 - val_accuracy: 0.3226\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5027 - accuracy: 0.3710 - val_loss: 0.5401 - val_accuracy: 0.3226\n",
      "1/1 [==============================] - 0s 52ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a simple neural network model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(df_labels.shape[1], activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions\n",
    "nn_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for extrovert (Neural Network): 0.8064516129032258\n",
      "Classification Report for extrovert (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      1.00      0.89        25\n",
      "         1.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.81        31\n",
      "   macro avg       0.40      0.50      0.45        31\n",
      "weighted avg       0.65      0.81      0.72        31\n",
      "\n",
      "Accuracy for feeling (Neural Network): 0.7419354838709677\n",
      "Classification Report for feeling (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      1.00      0.85        23\n",
      "         1.0       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.74        31\n",
      "   macro avg       0.37      0.50      0.43        31\n",
      "weighted avg       0.55      0.74      0.63        31\n",
      "\n",
      "Accuracy for judging (Neural Network): 0.5161290322580645\n",
      "Classification Report for judging (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      0.06      0.12        16\n",
      "         1.0       0.50      1.00      0.67        15\n",
      "\n",
      "    accuracy                           0.52        31\n",
      "   macro avg       0.75      0.53      0.39        31\n",
      "weighted avg       0.76      0.52      0.38        31\n",
      "\n",
      "Accuracy for sensing (Neural Network): 0.8709677419354839\n",
      "Classification Report for sensing (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93        27\n",
      "         1.0       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.87        31\n",
      "   macro avg       0.44      0.50      0.47        31\n",
      "weighted avg       0.76      0.87      0.81        31\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bella\\AppData\\Local\\Temp\\ipykernel_6576\\1734260993.py:2: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, true_labels in df_labels.iteritems():\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate for each df_lables\n",
    "for column, true_labels in df_labels.iteritems():\n",
    "    i = df_labels.columns.get_loc(column)  # Get the index of the current column\n",
    "    threshold = 0.5  # Adjust the threshold based on your task\n",
    "    binary_predictions = (nn_predictions[:, i] > threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_test[column], binary_predictions)\n",
    "    print(f\"Accuracy for {column} (Neural Network): {accuracy}\")\n",
    "\n",
    "    # Classification report for each dichotomy\n",
    "    print(f\"Classification Report for {column} (Neural Network):\")\n",
    "    print(classification_report(y_test[column], binary_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization \n",
    "\n",
    "def tokenize_documents(df):\n",
    "    tokenized_documents = []\n",
    "\n",
    "    for post in df['post_extrovert']:\n",
    "        token_list = []\n",
    "\n",
    "        for token in post.split(' '):\n",
    "            if token[-1] in string.punctuation:\n",
    "                token_list += [token[:-1], token[-1]]\n",
    "            else:\n",
    "                token_list.append(token)\n",
    "\n",
    "        tokenized_documents.append(token_list)\n",
    "\n",
    "    df['tokenized_post'] = tokenized_documents\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned = tokenize_documents(df_all_cleaned)\n",
    "df_all_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def vectorize_documents(df):\n",
    "    M = []\n",
    "    T = sorted(set([token for tokens in df['tokenized_post'] for token in tokens]))\n",
    "\n",
    "    for tokens in df['tokenized_post']:\n",
    "        vector = []\n",
    "        freq_dict = Counter(tokens)\n",
    "        \n",
    "        for t in T:\n",
    "            vector.append(freq_dict[t])\n",
    "\n",
    "        M.append(vector)\n",
    "\n",
    "    df['document_term_matrix'] = M\n",
    "    return df, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned, terms = vectorize_documents(df_all_cleaned)\n",
    "df_all_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # Assuming df_all_cleaned is your DataFrame with 'tokenized_post' column\n",
    "# documents = [' '.join(tokens) for tokens in df_all_cleaned['tokenized_post']]\n",
    "\n",
    "# Create a TF-IDF vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_all_cleaned['post_extrovert'])\n",
    "\n",
    "# Convert the TF-IDF matrix to a DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenate the TF-IDF DataFrame with the original DataFrame\n",
    "df_all_cleaned = pd.concat([df_all_cleaned, tfidf_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
