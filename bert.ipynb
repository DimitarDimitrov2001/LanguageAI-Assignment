{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Preprocessing:\n",
    "Clean and preprocess your text data as needed.\n",
    "Tokenize the text into words or subwords.\n",
    "\n",
    "2. BERT Embeddings:\n",
    "Use Hugging Face's Transformers library to load a pre-trained BERT model.\n",
    "Obtain contextualized word embeddings for each word in your text data.\n",
    "\n",
    "3.Topic Modeling:\n",
    "Apply a topic modeling technique (e.g., LDA or NMF) to extract latent topics from the preprocessed text data.\n",
    "\n",
    "4. Feature Extraction:\n",
    "Combine the BERT embeddings and topic distributions for each document. You can concatenate or merge these features.\n",
    "\n",
    "5. Model Training:\n",
    "Choose a classification model (e.g., logistic regression, random forest, or a neural network) for each dichotomy.\n",
    "Train each model on the combined feature matrix, using the corresponding labels.\n",
    "\n",
    "6. Evaluation:\n",
    "Evaluate the performance of each model on a testing set using appropriate metrics.\n",
    "\n",
    "7. Hyperparameter Tuning:\n",
    "Fine-tune the hyperparameters of each model to optimize performance.\n",
    "\n",
    "8. Interpretation:\n",
    "Analyze the importance of different features, including BERT embeddings and topic distributions, for each dichotomy to gain insights into how they contribute to personality type prediction.\n",
    "By building "
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
=======
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
>>>>>>> Stashed changes
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikeras.wrappers import KerasClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikeras\n",
      "  Obtaining dependency information for scikeras from https://files.pythonhosted.org/packages/5d/fa/9c1967952e7889d698f10ba8b6af79dfaa2e05178d97a79fbd9d1b44e589/scikeras-0.12.0-py3-none-any.whl.metadata\n",
      "  Downloading scikeras-0.12.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: packaging>=0.21 in c:\\users\\bella\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikeras) (23.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.0 in c:\\users\\bella\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikeras) (1.2.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem<0.32,>=0.23.1 in c:\\users\\bella\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikeras) (0.31.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\bella\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\bella\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\bella\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\bella\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn>=1.0.0->scikeras) (3.1.0)\n",
      "Downloading scikeras-0.12.0-py3-none-any.whl (27 kB)\n",
      "Installing collected packages: scikeras\n",
      "Successfully installed scikeras-0.12.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# pip install scikeras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned = pd.read_csv('data\\df_all_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 18,
=======
   "execution_count": 7,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auhtor_ID</th>\n",
       "      <th>post</th>\n",
       "      <th>extrovert</th>\n",
       "      <th>feeling</th>\n",
       "      <th>judging</th>\n",
       "      <th>sensing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_12bhu7</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_12jbpd</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_12uwr5</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_12zm15</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_13cjjl</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>t2_vfp8y</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>t2_w0842</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>t2_w6rgl</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>t2_wilcwvo</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t2_zq7gkv</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auhtor_ID                                               post  extrovert  \\\n",
       "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
       "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
       "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
       "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
       "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
       "..          ...                                                ...        ...   \n",
       "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
       "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
       "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
       "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
       "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
       "\n",
       "     feeling  judging  sensing  \n",
       "0        1.0      0.0      0.0  \n",
       "1        0.0      0.0      0.0  \n",
       "2        0.0      1.0      0.0  \n",
       "3        1.0      0.0      0.0  \n",
       "4        0.0      0.0      1.0  \n",
       "..       ...      ...      ...  \n",
       "150      0.0      1.0      0.0  \n",
       "151      0.0      1.0      0.0  \n",
       "152      0.0      1.0      0.0  \n",
       "153      0.0      0.0      0.0  \n",
       "154      1.0      1.0      0.0  \n",
       "\n",
       "[155 rows x 6 columns]"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 18,
=======
     "execution_count": 7,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_cleaned = df_all_cleaned.drop(['post_feeling', 'post_judging', 'post_sensing'], axis=1)\n",
    "df_all_cleaned = df_all_cleaned.rename(columns={'post_extrovert': 'post'})\n",
    "df_all_cleaned"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 19,
=======
   "execution_count": 9,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\20212397\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\20212397\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 20,
=======
   "execution_count": 10,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auhtor_ID</th>\n",
       "      <th>post</th>\n",
       "      <th>extrovert</th>\n",
       "      <th>feeling</th>\n",
       "      <th>judging</th>\n",
       "      <th>sensing</th>\n",
       "      <th>processed_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_12bhu7</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wear lorna shore shirt alot public lewd long s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_12jbpd</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>id say accurate characterization ni users read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_12uwr5</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ya know like people home decorations could sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_12zm15</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>true tho theyre kinda interesting buuuut issue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_13cjjl</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yeah thats one things make better objectively ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>t2_vfp8y</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>change profession would inadmissible country p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>t2_w0842</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technological singularity possibility contribu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>t2_w6rgl</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dear god man chill im einstein hawking serious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>t2_wilcwvo</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thats fake lib would say human blood water url...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t2_zq7gkv</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>biggest problem asking dont need amount recipr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auhtor_ID                                               post  extrovert  \\\n",
       "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
       "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
       "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
       "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
       "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
       "..          ...                                                ...        ...   \n",
       "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
       "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
       "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
       "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
       "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
       "\n",
       "     feeling  judging  sensing  \\\n",
       "0        1.0      0.0      0.0   \n",
       "1        0.0      0.0      0.0   \n",
       "2        0.0      1.0      0.0   \n",
       "3        1.0      0.0      0.0   \n",
       "4        0.0      0.0      1.0   \n",
       "..       ...      ...      ...   \n",
       "150      0.0      1.0      0.0   \n",
       "151      0.0      1.0      0.0   \n",
       "152      0.0      1.0      0.0   \n",
       "153      0.0      0.0      0.0   \n",
       "154      1.0      1.0      0.0   \n",
       "\n",
       "                                        processed_post  \n",
       "0    wear lorna shore shirt alot public lewd long s...  \n",
       "1    id say accurate characterization ni users read...  \n",
       "2    ya know like people home decorations could sav...  \n",
       "3    true tho theyre kinda interesting buuuut issue...  \n",
       "4    yeah thats one things make better objectively ...  \n",
       "..                                                 ...  \n",
       "150  change profession would inadmissible country p...  \n",
       "151  technological singularity possibility contribu...  \n",
       "152  dear god man chill im einstein hawking serious...  \n",
       "153  thats fake lib would say human blood water url...  \n",
       "154  biggest problem asking dont need amount recipr...  \n",
       "\n",
       "[155 rows x 7 columns]"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 20,
=======
     "execution_count": 10,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_cleaned['processed_post'] = df_all_cleaned['post'].apply(preprocess_text)\n",
    "df_all_cleaned"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 21,
=======
   "execution_count": 11,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<?, ?B/s]\n",
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\20212397\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.38MB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 1.77MB/s]\n",
      "config.json: 100%|██████████| 570/570 [00:00<?, ?B/s] \n",
      "model.safetensors: 100%|██████████| 440M/440M [00:18<00:00, 24.3MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      auhtor_ID                                               post  extrovert  \\\n",
      "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
      "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
      "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
      "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
      "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
      "..          ...                                                ...        ...   \n",
      "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
      "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
      "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
      "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
      "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
      "\n",
      "     feeling  judging  sensing  \\\n",
      "0        1.0      0.0      0.0   \n",
      "1        0.0      0.0      0.0   \n",
      "2        0.0      1.0      0.0   \n",
      "3        1.0      0.0      0.0   \n",
      "4        0.0      0.0      1.0   \n",
      "..       ...      ...      ...   \n",
      "150      0.0      1.0      0.0   \n",
      "151      0.0      1.0      0.0   \n",
      "152      0.0      1.0      0.0   \n",
      "153      0.0      0.0      0.0   \n",
      "154      1.0      1.0      0.0   \n",
      "\n",
      "                                        processed_post  \\\n",
      "0    wear lorna shore shirt alot public lewd long s...   \n",
      "1    id say accurate characterization ni users read...   \n",
      "2    ya know like people home decorations could sav...   \n",
      "3    true tho theyre kinda interesting buuuut issue...   \n",
      "4    yeah thats one things make better objectively ...   \n",
      "..                                                 ...   \n",
      "150  change profession would inadmissible country p...   \n",
      "151  technological singularity possibility contribu...   \n",
      "152  dear god man chill im einstein hawking serious...   \n",
      "153  thats fake lib would say human blood water url...   \n",
      "154  biggest problem asking dont need amount recipr...   \n",
      "\n",
      "                                       bert_embeddings  \n",
      "0    [0.037400726, 0.03744322, 0.40402812, -0.15458...  \n",
      "1    [-0.12263443, 0.06978707, 0.23516819, -0.16746...  \n",
      "2    [0.1035808, -0.07981775, 0.48626775, 0.0068327...  \n",
      "3    [-0.11131683, 0.070212886, 0.51686287, 0.01720...  \n",
      "4    [0.21926501, 0.11031015, 0.2861948, 0.10730274...  \n",
      "..                                                 ...  \n",
      "150  [-0.17524561, 0.18416233, 0.44777521, -0.11897...  \n",
      "151  [-0.025105802, -0.08084059, 0.3463778, -0.0189...  \n",
      "152  [0.088415354, 0.22571889, 0.38455784, -0.03194...  \n",
      "153  [0.059574068, 0.1017659, 0.38396204, -0.074290...  \n",
      "154  [-0.06220877, 0.0786678, 0.22912213, 0.0021381...  \n",
      "\n",
      "[155 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to obtain BERT embeddings for a text\n",
    "def get_bert_embeddings(text):\n",
    "    # Tokenize input text and convert to tensor\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "    # Get BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(tokens)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "\n",
    "    # Average the embeddings across tokens (you can modify this based on your needs)\n",
    "    avg_embedding = torch.mean(embeddings, dim=1).squeeze().numpy()\n",
    "\n",
    "    return avg_embedding\n",
    "\n",
    "df_all_cleaned['bert_embeddings'] = df_all_cleaned['post'].apply(get_bert_embeddings)\n",
    "print(df_all_cleaned)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 22,
=======
   "execution_count": 12,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auhtor_ID</th>\n",
       "      <th>post</th>\n",
       "      <th>extrovert</th>\n",
       "      <th>feeling</th>\n",
       "      <th>judging</th>\n",
       "      <th>sensing</th>\n",
       "      <th>processed_post</th>\n",
       "      <th>bert_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_12bhu7</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wear lorna shore shirt alot public lewd long s...</td>\n",
       "      <td>[0.037400726, 0.03744322, 0.40402812, -0.15458...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_12jbpd</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>id say accurate characterization ni users read...</td>\n",
       "      <td>[-0.12263443, 0.06978707, 0.23516819, -0.16746...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_12uwr5</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ya know like people home decorations could sav...</td>\n",
       "      <td>[0.1035808, -0.07981775, 0.48626775, 0.0068327...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_12zm15</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>true tho theyre kinda interesting buuuut issue...</td>\n",
       "      <td>[-0.11131683, 0.070212886, 0.51686287, 0.01720...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_13cjjl</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yeah thats one things make better objectively ...</td>\n",
       "      <td>[0.21926501, 0.11031015, 0.2861948, 0.10730274...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>t2_vfp8y</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>change profession would inadmissible country p...</td>\n",
       "      <td>[-0.17524561, 0.18416233, 0.44777521, -0.11897...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>t2_w0842</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technological singularity possibility contribu...</td>\n",
       "      <td>[-0.025105802, -0.08084059, 0.3463778, -0.0189...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>t2_w6rgl</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dear god man chill im einstein hawking serious...</td>\n",
       "      <td>[0.088415354, 0.22571889, 0.38455784, -0.03194...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>t2_wilcwvo</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thats fake lib would say human blood water url...</td>\n",
       "      <td>[0.059574068, 0.1017659, 0.38396204, -0.074290...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t2_zq7gkv</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>biggest problem asking dont need amount recipr...</td>\n",
       "      <td>[-0.06220877, 0.0786678, 0.22912213, 0.0021381...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auhtor_ID                                               post  extrovert  \\\n",
       "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
       "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
       "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
       "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
       "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
       "..          ...                                                ...        ...   \n",
       "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
       "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
       "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
       "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
       "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
       "\n",
       "     feeling  judging  sensing  \\\n",
       "0        1.0      0.0      0.0   \n",
       "1        0.0      0.0      0.0   \n",
       "2        0.0      1.0      0.0   \n",
       "3        1.0      0.0      0.0   \n",
       "4        0.0      0.0      1.0   \n",
       "..       ...      ...      ...   \n",
       "150      0.0      1.0      0.0   \n",
       "151      0.0      1.0      0.0   \n",
       "152      0.0      1.0      0.0   \n",
       "153      0.0      0.0      0.0   \n",
       "154      1.0      1.0      0.0   \n",
       "\n",
       "                                        processed_post  \\\n",
       "0    wear lorna shore shirt alot public lewd long s...   \n",
       "1    id say accurate characterization ni users read...   \n",
       "2    ya know like people home decorations could sav...   \n",
       "3    true tho theyre kinda interesting buuuut issue...   \n",
       "4    yeah thats one things make better objectively ...   \n",
       "..                                                 ...   \n",
       "150  change profession would inadmissible country p...   \n",
       "151  technological singularity possibility contribu...   \n",
       "152  dear god man chill im einstein hawking serious...   \n",
       "153  thats fake lib would say human blood water url...   \n",
       "154  biggest problem asking dont need amount recipr...   \n",
       "\n",
       "                                       bert_embeddings  \n",
       "0    [0.037400726, 0.03744322, 0.40402812, -0.15458...  \n",
       "1    [-0.12263443, 0.06978707, 0.23516819, -0.16746...  \n",
       "2    [0.1035808, -0.07981775, 0.48626775, 0.0068327...  \n",
       "3    [-0.11131683, 0.070212886, 0.51686287, 0.01720...  \n",
       "4    [0.21926501, 0.11031015, 0.2861948, 0.10730274...  \n",
       "..                                                 ...  \n",
       "150  [-0.17524561, 0.18416233, 0.44777521, -0.11897...  \n",
       "151  [-0.025105802, -0.08084059, 0.3463778, -0.0189...  \n",
       "152  [0.088415354, 0.22571889, 0.38455784, -0.03194...  \n",
       "153  [0.059574068, 0.1017659, 0.38396204, -0.074290...  \n",
       "154  [-0.06220877, 0.0786678, 0.22912213, 0.0021381...  \n",
       "\n",
       "[155 rows x 8 columns]"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 22,
=======
     "execution_count": 12,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.decomposition import LatentDirichletAllocation\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# # Function to apply LDA and return topic distributions\n",
    "# def get_topic_distribution(text):\n",
    "#     vectorizer = CountVectorizer(max_features=1000, stop_words='english')\n",
    "#     text_vectorized = vectorizer.fit_transform([text])\n",
    "    \n",
    "#     lda = LatentDirichletAllocation(n_components=5, random_state=42)  # Adjust the number of topics as needed\n",
    "#     topic_distribution = lda.fit_transform(text_vectorized)\n",
    "    \n",
    "#     return topic_distribution.flatten()\n",
    "\n",
    "# # Assume 'processed_post' is the column with processed text\n",
    "# df_all_cleaned['topic_distribution'] = df_all_cleaned['processed_post'].apply(get_topic_distribution)\n",
    "\n",
    "# # Display the DataFrame with combined features\n",
    "# print(df_all_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Combine BERT embeddings and topic distributions\n",
    "# # df_all_cleaned['combined_features'] = df_all_cleaned.apply(lambda row: np.concatenate([row['bert_embeddings'], row['topic_distribution']]), axis=1)\n",
    "\n",
    "# df_all_cleaned['combined_features'] = df_all_cleaned.apply(lambda row: 0.5 * (row['bert_embeddings'] + row['topic_distribution']), axis=1)\n",
    "\n",
    "\n",
    "# # Display the DataFrame with combined features\n",
    "# print(df_all_cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 23,
=======
   "execution_count": 13,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
<<<<<<< Updated upstream
      "Wall time: 8.45 ms\n"
=======
      "Wall time: 9.99 ms\n"
>>>>>>> Stashed changes
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extrovert</th>\n",
       "      <th>feeling</th>\n",
       "      <th>judging</th>\n",
       "      <th>sensing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     extrovert  feeling  judging  sensing\n",
       "0          1.0      1.0      0.0      0.0\n",
       "1          1.0      0.0      0.0      0.0\n",
       "2          0.0      0.0      1.0      0.0\n",
       "3          0.0      1.0      0.0      0.0\n",
       "4          0.0      0.0      0.0      1.0\n",
       "..         ...      ...      ...      ...\n",
       "150        0.0      0.0      1.0      0.0\n",
       "151        0.0      0.0      1.0      0.0\n",
       "152        0.0      0.0      1.0      0.0\n",
       "153        1.0      0.0      0.0      0.0\n",
       "154        1.0      1.0      1.0      0.0\n",
       "\n",
       "[155 rows x 4 columns]"
      ]
     },
<<<<<<< Updated upstream
     "execution_count": 23,
=======
     "execution_count": 13,
>>>>>>> Stashed changes
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_labels =  pd.DataFrame()\n",
    "df_labels['extrovert']= df_all_cleaned['extrovert']\n",
    "df_labels['feeling']= df_all_cleaned['feeling']\n",
    "df_labels['judging']= df_all_cleaned['judging']\n",
    "df_labels['sensing']= df_all_cleaned['sensing']\n",
    "\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 66,
=======
   "execution_count": 14,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "Epoch 1/10\n",
      "4/4 [==============================] - 1s 56ms/step - loss: 0.6033 - accuracy: 0.3145 - val_loss: 0.5624 - val_accuracy: 0.3226\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5625 - accuracy: 0.3710 - val_loss: 0.5475 - val_accuracy: 0.3226\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5464 - accuracy: 0.3710 - val_loss: 0.5461 - val_accuracy: 0.3226\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5307 - accuracy: 0.3710 - val_loss: 0.5547 - val_accuracy: 0.3226\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.5245 - accuracy: 0.3710 - val_loss: 0.5478 - val_accuracy: 0.3226\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.5161 - accuracy: 0.3710 - val_loss: 0.5484 - val_accuracy: 0.3226\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 9ms/step - loss: 0.5069 - accuracy: 0.3710 - val_loss: 0.5385 - val_accuracy: 0.3548\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4997 - accuracy: 0.3790 - val_loss: 0.5488 - val_accuracy: 0.3548\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4923 - accuracy: 0.3790 - val_loss: 0.5479 - val_accuracy: 0.3548\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4819 - accuracy: 0.3871 - val_loss: 0.5477 - val_accuracy: 0.3871\n",
      "1/1 [==============================] - 0s 49ms/step\n"
=======
      "WARNING:tensorflow:From c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "4/4 [==============================] - 1s 75ms/step - loss: 0.6035 - accuracy: 0.3629 - val_loss: 0.5544 - val_accuracy: 0.3226\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5484 - accuracy: 0.3710 - val_loss: 0.5606 - val_accuracy: 0.3226\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5374 - accuracy: 0.3710 - val_loss: 0.5596 - val_accuracy: 0.3226\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5275 - accuracy: 0.3710 - val_loss: 0.5572 - val_accuracy: 0.3226\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5249 - accuracy: 0.3710 - val_loss: 0.5645 - val_accuracy: 0.3226\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5088 - accuracy: 0.3710 - val_loss: 0.5422 - val_accuracy: 0.3226\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.5022 - accuracy: 0.3710 - val_loss: 0.5432 - val_accuracy: 0.3226\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4920 - accuracy: 0.3710 - val_loss: 0.5547 - val_accuracy: 0.3226\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4837 - accuracy: 0.3710 - val_loss: 0.5445 - val_accuracy: 0.3871\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 16ms/step - loss: 0.4735 - accuracy: 0.3710 - val_loss: 0.5429 - val_accuracy: 0.3548\n",
      "1/1 [==============================] - 0s 88ms/step\n"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "X = np.vstack(df_all_cleaned['bert_embeddings'])\n",
    "y = df_labels\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a simple neural network model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(df_labels.shape[1], activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions\n",
    "nn_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< Updated upstream
   "execution_count": 96,
=======
   "execution_count": 19,
>>>>>>> Stashed changes
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for extrovert (Neural Network): 0.7741935483870968\n",
      "Classification Report for extrovert (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.80      0.96      0.87        25\n",
      "         1.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.77        31\n",
      "   macro avg       0.40      0.48      0.44        31\n",
      "weighted avg       0.65      0.77      0.70        31\n",
      "\n",
      "Accuracy for feeling (Neural Network): 0.7419354838709677\n",
      "Classification Report for feeling (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      1.00      0.85        23\n",
      "         1.0       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.74        31\n",
      "   macro avg       0.37      0.50      0.43        31\n",
      "weighted avg       0.55      0.74      0.63        31\n",
      "\n",
      "Accuracy for judging (Neural Network): 0.41935483870967744\n",
      "Classification Report for judging (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.25      0.06      0.10        16\n",
      "         1.0       0.44      0.80      0.57        15\n",
      "\n",
      "    accuracy                           0.42        31\n",
      "   macro avg       0.35      0.43      0.34        31\n",
      "weighted avg       0.34      0.42      0.33        31\n",
      "\n",
      "Accuracy for sensing (Neural Network): 0.8709677419354839\n",
      "Classification Report for sensing (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93        27\n",
      "         1.0       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.87        31\n",
      "   macro avg       0.44      0.50      0.47        31\n",
      "weighted avg       0.76      0.87      0.81        31\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< Updated upstream
      "C:\\Users\\bella\\AppData\\Local\\Temp\\ipykernel_29812\\1734260993.py:2: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, true_labels in df_labels.iteritems():\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
=======
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
>>>>>>> Stashed changes
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
<<<<<<< Updated upstream
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate for each df_lables\n",
    "for column, true_labels in df_labels.iteritems():\n",
    "    i = df_labels.columns.get_loc(column)  # Get the index of the current column\n",
    "    threshold = 0.5  # Adjust the threshold based on your task\n",
    "    binary_predictions = (nn_predictions[:, i] > threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_test[column], binary_predictions)\n",
    "    print(f\"Accuracy for {column} (Neural Network): {accuracy}\")\n",
    "\n",
    "    # Classification report for each dichotomy\n",
    "    print(f\"Classification Report for {column} (Neural Network):\")\n",
    "    print(classification_report(y_test[column], binary_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.396245 using {'batch_size': 60, 'epochs': 100}\n",
      "0.363144 (0.036560) with: {'batch_size': 10, 'epochs': 10}\n",
      "0.363724 (0.074701) with: {'batch_size': 10, 'epochs': 50}\n",
      "0.363531 (0.055918) with: {'batch_size': 10, 'epochs': 100}\n",
      "0.371274 (0.048022) with: {'batch_size': 20, 'epochs': 10}\n",
      "0.379791 (0.067263) with: {'batch_size': 20, 'epochs': 50}\n",
      "0.379791 (0.067263) with: {'batch_size': 20, 'epochs': 100}\n",
      "0.371274 (0.048022) with: {'batch_size': 40, 'epochs': 10}\n",
      "0.346690 (0.021242) with: {'batch_size': 40, 'epochs': 50}\n",
      "0.380178 (0.100960) with: {'batch_size': 40, 'epochs': 100}\n",
      "0.371274 (0.048022) with: {'batch_size': 60, 'epochs': 10}\n",
      "0.396051 (0.083543) with: {'batch_size': 60, 'epochs': 50}\n",
      "0.396245 (0.097058) with: {'batch_size': 60, 'epochs': 100}\n",
      "0.371274 (0.048022) with: {'batch_size': 80, 'epochs': 10}\n",
      "0.330623 (0.010140) with: {'batch_size': 80, 'epochs': 50}\n",
      "0.322493 (0.021339) with: {'batch_size': 80, 'epochs': 100}\n",
      "0.371274 (0.048022) with: {'batch_size': 100, 'epochs': 10}\n",
      "0.362950 (0.020333) with: {'batch_size': 100, 'epochs': 50}\n",
      "0.395470 (0.033632) with: {'batch_size': 100, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(64, activation='sigmoid'))\n",
    "    model.add(Dense(df_labels.shape[1], activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.387921 using {'optimizer': 'Nadam'}\n",
      "0.371274 (0.048022) with: {'optimizer': 'SGD'}\n",
      "0.347271 (0.066131) with: {'optimizer': 'RMSprop'}\n",
      "0.371274 (0.048022) with: {'optimizer': 'Adagrad'}\n",
      "0.260163 (0.188225) with: {'optimizer': 'Adadelta'}\n",
      "0.379597 (0.053402) with: {'optimizer': 'Adam'}\n",
      "0.371080 (0.014010) with: {'optimizer': 'Adamax'}\n",
      "0.387921 (0.072271) with: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(64, activation='sigmoid'))\n",
    "    model.add(Dense(df_labels.shape[1], activation='sigmoid'))\n",
    "    # Compile model\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, loss=\"binary_crossentropy\", epochs= 100, batch_size =60, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m param_grid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(optimizer__learning_rate\u001b[38;5;241m=\u001b[39mlearn_rate, optimizer__momentum\u001b[38;5;241m=\u001b[39mmomentum)\n\u001b[0;32m     34\u001b[0m grid \u001b[38;5;241m=\u001b[39m GridSearchCV(estimator\u001b[38;5;241m=\u001b[39mmodel, param_grid\u001b[38;5;241m=\u001b[39mparam_grid, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m grid_result \u001b[38;5;241m=\u001b[39m \u001b[43mgrid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# summarize results\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest: \u001b[39m\u001b[38;5;132;01m%f\u001b[39;00m\u001b[38;5;124m using \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (grid_result\u001b[38;5;241m.\u001b[39mbest_score_, grid_result\u001b[38;5;241m.\u001b[39mbest_params_))\n",
      "File \u001b[1;32mc:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    976\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py:451\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m    449\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m__get_result()\n\u001b[1;32m--> 451\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_condition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mc:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py:320\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    318\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 320\u001b[0m         \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    321\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(64, activation='sigmoid'))\n",
    "    model.add(Dense(df_labels.shape[1], activation='sigmoid'))\n",
    "    # Compile model\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, loss=\"binary_crossentropy\", optimizer=\"SGD\", epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "param_grid = dict(optimizer__learning_rate=learn_rate, optimizer__momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.412118 using {'model__init_mode': 'he_normal'}\n",
      "0.363724 (0.074701) with: {'model__init_mode': 'uniform'}\n",
      "0.371854 (0.078381) with: {'model__init_mode': 'lecun_uniform'}\n",
      "0.379791 (0.067263) with: {'model__init_mode': 'normal'}\n",
      "0.387727 (0.056188) with: {'model__init_mode': 'zero'}\n",
      "0.379210 (0.015604) with: {'model__init_mode': 'glorot_normal'}\n",
      "0.363724 (0.071997) with: {'model__init_mode': 'glorot_uniform'}\n",
      "0.412118 (0.075229) with: {'model__init_mode': 'he_normal'}\n",
      "0.371467 (0.044918) with: {'model__init_mode': 'he_uniform'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "def create_model(init_mode='uniform'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(64, activation='sigmoid'))\n",
    "    model.add(Dense(df_labels.shape[1], activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=60, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(model__init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.403988 using {'model__activation': 'relu'}\n",
      "0.371274 (0.048022) with: {'model__activation': 'softmax'}\n",
      "0.379404 (0.044199) with: {'model__activation': 'softplus'}\n",
      "0.379597 (0.049549) with: {'model__activation': 'softsign'}\n",
      "0.403988 (0.067534) with: {'model__activation': 'relu'}\n",
      "0.355401 (0.053148) with: {'model__activation': 'tanh'}\n",
      "0.346883 (0.050700) with: {'model__activation': 'sigmoid'}\n",
      "0.379597 (0.053402) with: {'model__activation': 'hard_sigmoid'}\n",
      "0.379791 (0.067263) with: {'model__activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "def create_model(activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, kernel_initializer='he_uniform', activation=activation, input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(64, kernel_initializer='he_uniform', activation='sigmoid'))\n",
    "    model.add(Dense(df_labels.shape[1], activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=60, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(model__activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.404181 using {'model__dropout_rate': 0.0, 'model__weight_constraint': 1.0}\n",
      "0.404181 (0.083769) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 1.0}\n",
      "0.395664 (0.045185) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 2.0}\n",
      "0.379791 (0.067263) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 3.0}\n",
      "0.379597 (0.049549) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 4.0}\n",
      "0.355594 (0.072672) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 5.0}\n",
      "0.396051 (0.078653) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 1.0}\n",
      "0.395664 (0.045185) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 2.0}\n",
      "0.396245 (0.097058) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 3.0}\n",
      "0.371274 (0.026828) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 4.0}\n",
      "0.379985 (0.083495) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 5.0}\n",
      "0.371467 (0.044918) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 1.0}\n",
      "0.355401 (0.053148) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 2.0}\n",
      "0.403794 (0.049823) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 3.0}\n",
      "0.396051 (0.078653) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 4.0}\n",
      "0.379791 (0.072921) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 5.0}\n",
      "0.403988 (0.067534) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 1.0}\n",
      "0.379985 (0.083495) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 2.0}\n",
      "0.347271 (0.044651) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 3.0}\n",
      "0.403794 (0.049823) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 4.0}\n",
      "0.395858 (0.064213) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 5.0}\n",
      "0.371467 (0.053017) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 1.0}\n",
      "0.363337 (0.042948) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 2.0}\n",
      "0.347271 (0.044651) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 3.0}\n",
      "0.388115 (0.102192) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 4.0}\n",
      "0.395858 (0.085417) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 5.0}\n",
      "0.395470 (0.033632) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 1.0}\n",
      "0.379985 (0.083495) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 2.0}\n",
      "0.355788 (0.085571) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 3.0}\n",
      "0.371661 (0.072662) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 4.0}\n",
      "0.363724 (0.071997) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 5.0}\n",
      "0.396245 (0.094993) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 1.0}\n",
      "0.355594 (0.066993) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 2.0}\n",
      "0.355207 (0.043998) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 3.0}\n",
      "0.347077 (0.047869) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 4.0}\n",
      "0.379985 (0.085837) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 5.0}\n",
      "0.314750 (0.036247) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 1.0}\n",
      "0.331204 (0.063147) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 2.0}\n",
      "0.379985 (0.085837) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 3.0}\n",
      "0.298684 (0.032756) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 4.0}\n",
      "0.355594 (0.066993) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 5.0}\n",
      "0.322687 (0.031288) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 1.0}\n",
      "0.355014 (0.025132) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 2.0}\n",
      "0.347077 (0.026554) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 3.0}\n",
      "0.363144 (0.036560) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 4.0}\n",
      "0.355014 (0.025132) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 5.0}\n",
      "0.363144 (0.036560) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 1.0}\n",
      "0.363144 (0.036560) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 2.0}\n",
      "0.363144 (0.036560) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 3.0}\n",
      "0.363144 (0.036560) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 4.0}\n",
      "0.346883 (0.013818) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 5.0}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "def create_model(dropout_rate, weight_constraint):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, kernel_initializer='he_uniform', activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(64, kernel_initializer='he_uniform', activation='sigmoid'))\n",
    "    model.add(Dense(df_labels.shape[1], activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=60, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "weight_constraint = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(model__dropout_rate=dropout_rate, model__weight_constraint=weight_constraint)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.403794 using {'model__neurons': 30}\n",
      "0.379404 (0.059497) with: {'model__neurons': 1}\n",
      "0.355014 (0.025132) with: {'model__neurons': 5}\n",
      "0.355014 (0.025132) with: {'model__neurons': 10}\n",
      "0.355014 (0.025132) with: {'model__neurons': 15}\n",
      "0.371661 (0.085222) with: {'model__neurons': 20}\n",
      "0.331010 (0.043797) with: {'model__neurons': 25}\n",
      "0.403794 (0.053656) with: {'model__neurons': 30}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "def create_model(neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_shape=(X_train.shape[1],), kernel_initializer='he_uniform', activation='relu', kernel_constraint=MaxNorm(1, axis=0)))\n",
    "    model.add(Dropout(0.0))\n",
    "    model.add(Dense(64, kernel_initializer='he_uniform', activation='sigmoid'))\n",
    "    model.add(Dense(df_labels.shape[1], activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=60, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "param_grid = dict(model__neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x0000018BFA3B7D80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import tensorflow \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "\n",
    "\n",
    "# X = np.vstack(df_all_cleaned['bert_embeddings'])\n",
    "# y = df_labels\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, df_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a simple neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=1, input_shape=(X_train.shape[1],), kernel_initializer='he_uniform', activation='relu', kernel_constraint=MaxNorm(1, axis=0)))\n",
    "    model.add(Dropout(0.0))\n",
    "    model.add(Dense(64, kernel_initializer='he_uniform', activation='sigmoid'))\n",
    "    model.add(Dense(df_labels.shape[1], activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=60, verbose=0)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=100, batch_size=60, validation_data=(X_test, y_test))\n",
    "# \n",
    "# Predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for extrovert (Neural Network): 0.8064516129032258\n",
      "Classification Report for extrovert (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      1.00      0.89        25\n",
      "         1.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.81        31\n",
      "   macro avg       0.40      0.50      0.45        31\n",
      "weighted avg       0.65      0.81      0.72        31\n",
      "\n",
      "Accuracy for feeling (Neural Network): 0.7419354838709677\n",
      "Classification Report for feeling (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      1.00      0.85        23\n",
      "         1.0       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.74        31\n",
      "   macro avg       0.37      0.50      0.43        31\n",
      "weighted avg       0.55      0.74      0.63        31\n",
      "\n",
      "Accuracy for judging (Neural Network): 0.4838709677419355\n",
      "Classification Report for judging (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        16\n",
      "         1.0       0.48      1.00      0.65        15\n",
      "\n",
      "    accuracy                           0.48        31\n",
      "   macro avg       0.24      0.50      0.33        31\n",
      "weighted avg       0.23      0.48      0.32        31\n",
      "\n",
      "Accuracy for sensing (Neural Network): 0.8709677419354839\n",
      "Classification Report for sensing (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93        27\n",
      "         1.0       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.87        31\n",
      "   macro avg       0.44      0.50      0.47        31\n",
      "weighted avg       0.76      0.87      0.81        31\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bella\\AppData\\Local\\Temp\\ipykernel_29812\\1440492696.py:2: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, true_labels in df_labels.iteritems():\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
=======
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
>>>>>>> Stashed changes
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate for each df_lables\n",
    "for column, true_labels in df_labels.items():\n",
    "    i = df_labels.columns.get_loc(column)  # Get the index of the current column\n",
    "    threshold = 0.5  # Adjust the threshold based on your task\n",
    "    binary_predictions = (predictions[:, i] > threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_test[column], binary_predictions)\n",
    "    print(f\"Accuracy for {column} (Neural Network): {accuracy}\")\n",
    "\n",
    "    # Classification report for each dichotomy\n",
    "    print(f\"Classification Report for {column} (Neural Network):\")\n",
    "    print(classification_report(y_test[column], binary_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization \n",
    "\n",
    "def tokenize_documents(df):\n",
    "    tokenized_documents = []\n",
    "\n",
    "    for post in df['post']:\n",
    "        token_list = []\n",
    "\n",
    "        for token in post.split(' '):\n",
    "            if token[-1] in string.punctuation:\n",
    "                token_list += [token[:-1], token[-1]]\n",
    "            else:\n",
    "                token_list.append(token)\n",
    "\n",
    "        tokenized_documents.append(token_list)\n",
    "\n",
    "    df['tokenized_post'] = tokenized_documents\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auhtor_ID</th>\n",
       "      <th>post</th>\n",
       "      <th>extrovert</th>\n",
       "      <th>feeling</th>\n",
       "      <th>judging</th>\n",
       "      <th>sensing</th>\n",
       "      <th>processed_post</th>\n",
       "      <th>bert_embeddings</th>\n",
       "      <th>tokenized_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_12bhu7</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wear lorna shore shirt alot public lewd long s...</td>\n",
       "      <td>[0.037400726, 0.03744322, 0.40402812, -0.15458...</td>\n",
       "      <td>[I, wear, a, Lorna, shore, shirt, out, alot, i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_12jbpd</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>id say accurate characterization ni users read...</td>\n",
       "      <td>[-0.12263443, 0.06978707, 0.23516819, -0.16746...</td>\n",
       "      <td>[I'd, say, this, is, a, very, accurate, charac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_12uwr5</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ya know like people home decorations could sav...</td>\n",
       "      <td>[0.1035808, -0.07981775, 0.48626775, 0.0068327...</td>\n",
       "      <td>[Ya, know, like, most, people, with, home, dec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_12zm15</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>true tho theyre kinda interesting buuuut issue...</td>\n",
       "      <td>[-0.11131683, 0.070212886, 0.51686287, 0.01720...</td>\n",
       "      <td>[It's, true, tho, ., They're, kinda, more, int...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_13cjjl</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yeah thats one things make better objectively ...</td>\n",
       "      <td>[0.21926501, 0.11031015, 0.2861948, 0.10730274...</td>\n",
       "      <td>[Yeah, ,, but, that's, one, of, the, things, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>t2_vfp8y</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>change profession would inadmissible country p...</td>\n",
       "      <td>[-0.17524561, 0.18416233, 0.44777521, -0.11897...</td>\n",
       "      <td>[so, change, profession, then, ., this, would,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>t2_w0842</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technological singularity possibility contribu...</td>\n",
       "      <td>[-0.025105802, -0.08084059, 0.3463778, -0.0189...</td>\n",
       "      <td>[The, technological, singularity, ., And, the,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>t2_w6rgl</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dear god man chill im einstein hawking serious...</td>\n",
       "      <td>[0.088415354, 0.22571889, 0.38455784, -0.03194...</td>\n",
       "      <td>[Dear, God, man, ., Chill, ., I'm, not, Einste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>t2_wilcwvo</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thats fake lib would say human blood water url...</td>\n",
       "      <td>[0.059574068, 0.1017659, 0.38396204, -0.074290...</td>\n",
       "      <td>[That's, what, a, fake, lib, would, say, [Huma...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t2_zq7gkv</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>biggest problem asking dont need amount recipr...</td>\n",
       "      <td>[-0.06220877, 0.0786678, 0.22912213, 0.0021381...</td>\n",
       "      <td>[My, biggest, problem, is, asking, for, it, .,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auhtor_ID                                               post  extrovert  \\\n",
       "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
       "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
       "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
       "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
       "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
       "..          ...                                                ...        ...   \n",
       "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
       "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
       "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
       "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
       "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
       "\n",
       "     feeling  judging  sensing  \\\n",
       "0        1.0      0.0      0.0   \n",
       "1        0.0      0.0      0.0   \n",
       "2        0.0      1.0      0.0   \n",
       "3        1.0      0.0      0.0   \n",
       "4        0.0      0.0      1.0   \n",
       "..       ...      ...      ...   \n",
       "150      0.0      1.0      0.0   \n",
       "151      0.0      1.0      0.0   \n",
       "152      0.0      1.0      0.0   \n",
       "153      0.0      0.0      0.0   \n",
       "154      1.0      1.0      0.0   \n",
       "\n",
       "                                        processed_post  \\\n",
       "0    wear lorna shore shirt alot public lewd long s...   \n",
       "1    id say accurate characterization ni users read...   \n",
       "2    ya know like people home decorations could sav...   \n",
       "3    true tho theyre kinda interesting buuuut issue...   \n",
       "4    yeah thats one things make better objectively ...   \n",
       "..                                                 ...   \n",
       "150  change profession would inadmissible country p...   \n",
       "151  technological singularity possibility contribu...   \n",
       "152  dear god man chill im einstein hawking serious...   \n",
       "153  thats fake lib would say human blood water url...   \n",
       "154  biggest problem asking dont need amount recipr...   \n",
       "\n",
       "                                       bert_embeddings  \\\n",
       "0    [0.037400726, 0.03744322, 0.40402812, -0.15458...   \n",
       "1    [-0.12263443, 0.06978707, 0.23516819, -0.16746...   \n",
       "2    [0.1035808, -0.07981775, 0.48626775, 0.0068327...   \n",
       "3    [-0.11131683, 0.070212886, 0.51686287, 0.01720...   \n",
       "4    [0.21926501, 0.11031015, 0.2861948, 0.10730274...   \n",
       "..                                                 ...   \n",
       "150  [-0.17524561, 0.18416233, 0.44777521, -0.11897...   \n",
       "151  [-0.025105802, -0.08084059, 0.3463778, -0.0189...   \n",
       "152  [0.088415354, 0.22571889, 0.38455784, -0.03194...   \n",
       "153  [0.059574068, 0.1017659, 0.38396204, -0.074290...   \n",
       "154  [-0.06220877, 0.0786678, 0.22912213, 0.0021381...   \n",
       "\n",
       "                                        tokenized_post  \n",
       "0    [I, wear, a, Lorna, shore, shirt, out, alot, i...  \n",
       "1    [I'd, say, this, is, a, very, accurate, charac...  \n",
       "2    [Ya, know, like, most, people, with, home, dec...  \n",
       "3    [It's, true, tho, ., They're, kinda, more, int...  \n",
       "4    [Yeah, ,, but, that's, one, of, the, things, t...  \n",
       "..                                                 ...  \n",
       "150  [so, change, profession, then, ., this, would,...  \n",
       "151  [The, technological, singularity, ., And, the,...  \n",
       "152  [Dear, God, man, ., Chill, ., I'm, not, Einste...  \n",
       "153  [That's, what, a, fake, lib, would, say, [Huma...  \n",
       "154  [My, biggest, problem, is, asking, for, it, .,...  \n",
       "\n",
       "[155 rows x 9 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_all_cleaned = tokenize_documents(df_all_cleaned)\n",
    "# df_all_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorization\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "def vectorize_documents(df):\n",
    "    M = []\n",
    "    T = sorted(set([token for tokens in df['tokenized_post'] for token in tokens]))\n",
    "\n",
    "    for tokens in df['tokenized_post']:\n",
    "        vector = []\n",
    "        freq_dict = Counter(tokens)\n",
    "        \n",
    "        for t in T:\n",
    "            vector.append(freq_dict[t])\n",
    "\n",
    "        M.append(vector)\n",
    "\n",
    "    df['document_term_matrix'] = M\n",
    "    return df, T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auhtor_ID</th>\n",
       "      <th>post</th>\n",
       "      <th>extrovert</th>\n",
       "      <th>feeling</th>\n",
       "      <th>judging</th>\n",
       "      <th>sensing</th>\n",
       "      <th>processed_post</th>\n",
       "      <th>bert_embeddings</th>\n",
       "      <th>tokenized_post</th>\n",
       "      <th>document_term_matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_12bhu7</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wear lorna shore shirt alot public lewd long s...</td>\n",
       "      <td>[0.037400726, 0.03744322, 0.40402812, -0.15458...</td>\n",
       "      <td>[I, wear, a, Lorna, shore, shirt, out, alot, i...</td>\n",
       "      <td>[63, 99, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_12jbpd</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>id say accurate characterization ni users read...</td>\n",
       "      <td>[-0.12263443, 0.06978707, 0.23516819, -0.16746...</td>\n",
       "      <td>[I'd, say, this, is, a, very, accurate, charac...</td>\n",
       "      <td>[35, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_12uwr5</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ya know like people home decorations could sav...</td>\n",
       "      <td>[0.1035808, -0.07981775, 0.48626775, 0.0068327...</td>\n",
       "      <td>[Ya, know, like, most, people, with, home, dec...</td>\n",
       "      <td>[267, 383, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_12zm15</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>true tho theyre kinda interesting buuuut issue...</td>\n",
       "      <td>[-0.11131683, 0.070212886, 0.51686287, 0.01720...</td>\n",
       "      <td>[It's, true, tho, ., They're, kinda, more, int...</td>\n",
       "      <td>[30, 436, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_13cjjl</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yeah thats one things make better objectively ...</td>\n",
       "      <td>[0.21926501, 0.11031015, 0.2861948, 0.10730274...</td>\n",
       "      <td>[Yeah, ,, but, that's, one, of, the, things, t...</td>\n",
       "      <td>[6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>t2_vfp8y</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>change profession would inadmissible country p...</td>\n",
       "      <td>[-0.17524561, 0.18416233, 0.44777521, -0.11897...</td>\n",
       "      <td>[so, change, profession, then, ., this, would,...</td>\n",
       "      <td>[61, 111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>t2_w0842</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technological singularity possibility contribu...</td>\n",
       "      <td>[-0.025105802, -0.08084059, 0.3463778, -0.0189...</td>\n",
       "      <td>[The, technological, singularity, ., And, the,...</td>\n",
       "      <td>[23, 74, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>t2_w6rgl</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dear god man chill im einstein hawking serious...</td>\n",
       "      <td>[0.088415354, 0.22571889, 0.38455784, -0.03194...</td>\n",
       "      <td>[Dear, God, man, ., Chill, ., I'm, not, Einste...</td>\n",
       "      <td>[101, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>t2_wilcwvo</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thats fake lib would say human blood water url...</td>\n",
       "      <td>[0.059574068, 0.1017659, 0.38396204, -0.074290...</td>\n",
       "      <td>[That's, what, a, fake, lib, would, say, [Huma...</td>\n",
       "      <td>[67, 27, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t2_zq7gkv</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>biggest problem asking dont need amount recipr...</td>\n",
       "      <td>[-0.06220877, 0.0786678, 0.22912213, 0.0021381...</td>\n",
       "      <td>[My, biggest, problem, is, asking, for, it, .,...</td>\n",
       "      <td>[72, 147, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auhtor_ID                                               post  extrovert  \\\n",
       "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
       "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
       "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
       "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
       "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
       "..          ...                                                ...        ...   \n",
       "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
       "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
       "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
       "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
       "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
       "\n",
       "     feeling  judging  sensing  \\\n",
       "0        1.0      0.0      0.0   \n",
       "1        0.0      0.0      0.0   \n",
       "2        0.0      1.0      0.0   \n",
       "3        1.0      0.0      0.0   \n",
       "4        0.0      0.0      1.0   \n",
       "..       ...      ...      ...   \n",
       "150      0.0      1.0      0.0   \n",
       "151      0.0      1.0      0.0   \n",
       "152      0.0      1.0      0.0   \n",
       "153      0.0      0.0      0.0   \n",
       "154      1.0      1.0      0.0   \n",
       "\n",
       "                                        processed_post  \\\n",
       "0    wear lorna shore shirt alot public lewd long s...   \n",
       "1    id say accurate characterization ni users read...   \n",
       "2    ya know like people home decorations could sav...   \n",
       "3    true tho theyre kinda interesting buuuut issue...   \n",
       "4    yeah thats one things make better objectively ...   \n",
       "..                                                 ...   \n",
       "150  change profession would inadmissible country p...   \n",
       "151  technological singularity possibility contribu...   \n",
       "152  dear god man chill im einstein hawking serious...   \n",
       "153  thats fake lib would say human blood water url...   \n",
       "154  biggest problem asking dont need amount recipr...   \n",
       "\n",
       "                                       bert_embeddings  \\\n",
       "0    [0.037400726, 0.03744322, 0.40402812, -0.15458...   \n",
       "1    [-0.12263443, 0.06978707, 0.23516819, -0.16746...   \n",
       "2    [0.1035808, -0.07981775, 0.48626775, 0.0068327...   \n",
       "3    [-0.11131683, 0.070212886, 0.51686287, 0.01720...   \n",
       "4    [0.21926501, 0.11031015, 0.2861948, 0.10730274...   \n",
       "..                                                 ...   \n",
       "150  [-0.17524561, 0.18416233, 0.44777521, -0.11897...   \n",
       "151  [-0.025105802, -0.08084059, 0.3463778, -0.0189...   \n",
       "152  [0.088415354, 0.22571889, 0.38455784, -0.03194...   \n",
       "153  [0.059574068, 0.1017659, 0.38396204, -0.074290...   \n",
       "154  [-0.06220877, 0.0786678, 0.22912213, 0.0021381...   \n",
       "\n",
       "                                        tokenized_post  \\\n",
       "0    [I, wear, a, Lorna, shore, shirt, out, alot, i...   \n",
       "1    [I'd, say, this, is, a, very, accurate, charac...   \n",
       "2    [Ya, know, like, most, people, with, home, dec...   \n",
       "3    [It's, true, tho, ., They're, kinda, more, int...   \n",
       "4    [Yeah, ,, but, that's, one, of, the, things, t...   \n",
       "..                                                 ...   \n",
       "150  [so, change, profession, then, ., this, would,...   \n",
       "151  [The, technological, singularity, ., And, the,...   \n",
       "152  [Dear, God, man, ., Chill, ., I'm, not, Einste...   \n",
       "153  [That's, what, a, fake, lib, would, say, [Huma...   \n",
       "154  [My, biggest, problem, is, asking, for, it, .,...   \n",
       "\n",
       "                                  document_term_matrix  \n",
       "0    [63, 99, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "1    [35, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "2    [267, 383, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "3    [30, 436, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4    [6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "..                                                 ...  \n",
       "150  [61, 111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "151  [23, 74, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "152  [101, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "153  [67, 27, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  \n",
       "154  [72, 147, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  \n",
       "\n",
       "[155 rows x 10 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_all_cleaned, terms = vectorize_documents(df_all_cleaned)\n",
    "# df_all_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# # # Assuming df_all_cleaned is your DataFrame with 'tokenized_post' column\n",
    "# # documents = [' '.join(tokens) for tokens in df_all_cleaned['tokenized_post']]\n",
    "\n",
    "# # Create a TF-IDF vectorizer\n",
    "# tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
<<<<<<< Updated upstream
    "# # Fit and transform the documents\n",
    "# tfidf_matrix = tfidf_vectorizer.fit_transform(df_all_cleaned['post_extrovert'])\n",
=======
    "# Fit and transform the documents\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df_all_cleaned['post'])\n",
>>>>>>> Stashed changes
    "\n",
    "# # Convert the TF-IDF matrix to a DataFrame\n",
    "# tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
<<<<<<< Updated upstream
    "# # Concatenate the TF-IDF DataFrame with the original DataFrame\n",
    "# df_all_cleaned = pd.concat([df_all_cleaned, tfidf_df], axis=1)\n"
=======
    "# Concatenate the TF-IDF DataFrame with the original DataFrame\n",
    "df_all_cleaned = pd.concat([df_all_cleaned, tfidf_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auhtor_ID</th>\n",
       "      <th>post</th>\n",
       "      <th>extrovert</th>\n",
       "      <th>feeling</th>\n",
       "      <th>judging</th>\n",
       "      <th>sensing</th>\n",
       "      <th>processed_post</th>\n",
       "      <th>bert_embeddings</th>\n",
       "      <th>tokenized_post</th>\n",
       "      <th>document_term_matrix</th>\n",
       "      <th>...</th>\n",
       "      <th>𝐖𝐨𝐦𝐚𝐧</th>\n",
       "      <th>𝐧𝐭</th>\n",
       "      <th>𝐬𝐡</th>\n",
       "      <th>𝐹𝑠</th>\n",
       "      <th>𝑎𝑐𝑐𝑒𝑙𝑒𝑟𝑎𝑡𝑒</th>\n",
       "      <th>𝑓𝑜𝑟𝑐𝑒</th>\n",
       "      <th>𝑚𝑎</th>\n",
       "      <th>𝑚𝑎𝑠𝑠</th>\n",
       "      <th>𝑚𝑣</th>\n",
       "      <th>𝗪𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗚𝗛</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_12bhu7</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wear lorna shore shirt alot public lewd long s...</td>\n",
       "      <td>[0.037400726, 0.03744322, 0.40402812, -0.15458...</td>\n",
       "      <td>[I, wear, a, Lorna, shore, shirt, out, alot, i...</td>\n",
       "      <td>[63, 99, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_12jbpd</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>id say accurate characterization ni users read...</td>\n",
       "      <td>[-0.12263443, 0.06978707, 0.23516819, -0.16746...</td>\n",
       "      <td>[I'd, say, this, is, a, very, accurate, charac...</td>\n",
       "      <td>[35, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_12uwr5</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ya know like people home decorations could sav...</td>\n",
       "      <td>[0.1035808, -0.07981775, 0.48626775, 0.0068327...</td>\n",
       "      <td>[Ya, know, like, most, people, with, home, dec...</td>\n",
       "      <td>[267, 383, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_12zm15</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>true tho theyre kinda interesting buuuut issue...</td>\n",
       "      <td>[-0.11131683, 0.070212886, 0.51686287, 0.01720...</td>\n",
       "      <td>[It's, true, tho, ., They're, kinda, more, int...</td>\n",
       "      <td>[30, 436, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_13cjjl</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yeah thats one things make better objectively ...</td>\n",
       "      <td>[0.21926501, 0.11031015, 0.2861948, 0.10730274...</td>\n",
       "      <td>[Yeah, ,, but, that's, one, of, the, things, t...</td>\n",
       "      <td>[6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>t2_vfp8y</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>change profession would inadmissible country p...</td>\n",
       "      <td>[-0.17524561, 0.18416233, 0.44777521, -0.11897...</td>\n",
       "      <td>[so, change, profession, then, ., this, would,...</td>\n",
       "      <td>[61, 111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>t2_w0842</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technological singularity possibility contribu...</td>\n",
       "      <td>[-0.025105802, -0.08084059, 0.3463778, -0.0189...</td>\n",
       "      <td>[The, technological, singularity, ., And, the,...</td>\n",
       "      <td>[23, 74, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>t2_w6rgl</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dear god man chill im einstein hawking serious...</td>\n",
       "      <td>[0.088415354, 0.22571889, 0.38455784, -0.03194...</td>\n",
       "      <td>[Dear, God, man, ., Chill, ., I'm, not, Einste...</td>\n",
       "      <td>[101, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>t2_wilcwvo</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thats fake lib would say human blood water url...</td>\n",
       "      <td>[0.059574068, 0.1017659, 0.38396204, -0.074290...</td>\n",
       "      <td>[That's, what, a, fake, lib, would, say, [Huma...</td>\n",
       "      <td>[67, 27, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t2_zq7gkv</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>biggest problem asking dont need amount recipr...</td>\n",
       "      <td>[-0.06220877, 0.0786678, 0.22912213, 0.0021381...</td>\n",
       "      <td>[My, biggest, problem, is, asking, for, it, .,...</td>\n",
       "      <td>[72, 147, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 86759 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auhtor_ID                                               post  extrovert  \\\n",
       "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
       "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
       "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
       "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
       "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
       "..          ...                                                ...        ...   \n",
       "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
       "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
       "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
       "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
       "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
       "\n",
       "     feeling  judging  sensing  \\\n",
       "0        1.0      0.0      0.0   \n",
       "1        0.0      0.0      0.0   \n",
       "2        0.0      1.0      0.0   \n",
       "3        1.0      0.0      0.0   \n",
       "4        0.0      0.0      1.0   \n",
       "..       ...      ...      ...   \n",
       "150      0.0      1.0      0.0   \n",
       "151      0.0      1.0      0.0   \n",
       "152      0.0      1.0      0.0   \n",
       "153      0.0      0.0      0.0   \n",
       "154      1.0      1.0      0.0   \n",
       "\n",
       "                                        processed_post  \\\n",
       "0    wear lorna shore shirt alot public lewd long s...   \n",
       "1    id say accurate characterization ni users read...   \n",
       "2    ya know like people home decorations could sav...   \n",
       "3    true tho theyre kinda interesting buuuut issue...   \n",
       "4    yeah thats one things make better objectively ...   \n",
       "..                                                 ...   \n",
       "150  change profession would inadmissible country p...   \n",
       "151  technological singularity possibility contribu...   \n",
       "152  dear god man chill im einstein hawking serious...   \n",
       "153  thats fake lib would say human blood water url...   \n",
       "154  biggest problem asking dont need amount recipr...   \n",
       "\n",
       "                                       bert_embeddings  \\\n",
       "0    [0.037400726, 0.03744322, 0.40402812, -0.15458...   \n",
       "1    [-0.12263443, 0.06978707, 0.23516819, -0.16746...   \n",
       "2    [0.1035808, -0.07981775, 0.48626775, 0.0068327...   \n",
       "3    [-0.11131683, 0.070212886, 0.51686287, 0.01720...   \n",
       "4    [0.21926501, 0.11031015, 0.2861948, 0.10730274...   \n",
       "..                                                 ...   \n",
       "150  [-0.17524561, 0.18416233, 0.44777521, -0.11897...   \n",
       "151  [-0.025105802, -0.08084059, 0.3463778, -0.0189...   \n",
       "152  [0.088415354, 0.22571889, 0.38455784, -0.03194...   \n",
       "153  [0.059574068, 0.1017659, 0.38396204, -0.074290...   \n",
       "154  [-0.06220877, 0.0786678, 0.22912213, 0.0021381...   \n",
       "\n",
       "                                        tokenized_post  \\\n",
       "0    [I, wear, a, Lorna, shore, shirt, out, alot, i...   \n",
       "1    [I'd, say, this, is, a, very, accurate, charac...   \n",
       "2    [Ya, know, like, most, people, with, home, dec...   \n",
       "3    [It's, true, tho, ., They're, kinda, more, int...   \n",
       "4    [Yeah, ,, but, that's, one, of, the, things, t...   \n",
       "..                                                 ...   \n",
       "150  [so, change, profession, then, ., this, would,...   \n",
       "151  [The, technological, singularity, ., And, the,...   \n",
       "152  [Dear, God, man, ., Chill, ., I'm, not, Einste...   \n",
       "153  [That's, what, a, fake, lib, would, say, [Huma...   \n",
       "154  [My, biggest, problem, is, asking, for, it, .,...   \n",
       "\n",
       "                                  document_term_matrix  ...  𝐖𝐨𝐦𝐚𝐧   𝐧𝐭   𝐬𝐡  \\\n",
       "0    [63, 99, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  ...    0.0  0.0  0.0   \n",
       "1    [35, 23, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  ...    0.0  0.0  0.0   \n",
       "2    [267, 383, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,...  ...    0.0  0.0  0.0   \n",
       "3    [30, 436, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ...    0.0  0.0  0.0   \n",
       "4    [6, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ...    0.0  0.0  0.0   \n",
       "..                                                 ...  ...    ...  ...  ...   \n",
       "150  [61, 111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ...    0.0  0.0  0.0   \n",
       "151  [23, 74, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  ...    0.0  0.0  0.0   \n",
       "152  [101, 68, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  ...    0.0  0.0  0.0   \n",
       "153  [67, 27, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0...  ...    0.0  0.0  0.0   \n",
       "154  [72, 147, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...  ...    0.0  0.0  0.0   \n",
       "\n",
       "      𝐹𝑠  𝑎𝑐𝑐𝑒𝑙𝑒𝑟𝑎𝑡𝑒  𝑓𝑜𝑟𝑐𝑒   𝑚𝑎  𝑚𝑎𝑠𝑠   𝑚𝑣  𝗪𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗔𝗚𝗛  \n",
       "0    0.0         0.0    0.0  0.0   0.0  0.0                  0.0  \n",
       "1    0.0         0.0    0.0  0.0   0.0  0.0                  0.0  \n",
       "2    0.0         0.0    0.0  0.0   0.0  0.0                  0.0  \n",
       "3    0.0         0.0    0.0  0.0   0.0  0.0                  0.0  \n",
       "4    0.0         0.0    0.0  0.0   0.0  0.0                  0.0  \n",
       "..   ...         ...    ...  ...   ...  ...                  ...  \n",
       "150  0.0         0.0    0.0  0.0   0.0  0.0                  0.0  \n",
       "151  0.0         0.0    0.0  0.0   0.0  0.0                  0.0  \n",
       "152  0.0         0.0    0.0  0.0   0.0  0.0                  0.0  \n",
       "153  0.0         0.0    0.0  0.0   0.0  0.0                  0.0  \n",
       "154  0.0         0.0    0.0  0.0   0.0  0.0                  0.0  \n",
       "\n",
       "[155 rows x 86759 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_cleaned"
>>>>>>> Stashed changes
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
