{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Data Preprocessing:\n",
    "Clean and preprocess your text data as needed.\n",
    "Tokenize the text into words or subwords.\n",
    "\n",
    "2. BERT Embeddings:\n",
    "Use Hugging Face's Transformers library to load a pre-trained BERT model.\n",
    "Obtain contextualized word embeddings for each word in your text data.\n",
    "\n",
    "3.Topic Modeling:\n",
    "Apply a topic modeling technique (e.g., LDA or NMF) to extract latent topics from the preprocessed text data.\n",
    "\n",
    "4. Feature Extraction:\n",
    "Combine the BERT embeddings and topic distributions for each document. You can concatenate or merge these features.\n",
    "\n",
    "5. Model Training:\n",
    "Choose a classification model (e.g., logistic regression, random forest, or a neural network) for each dichotomy.\n",
    "Train each model on the combined feature matrix, using the corresponding labels.\n",
    "\n",
    "6. Evaluation:\n",
    "Evaluate the performance of each model on a testing set using appropriate metrics.\n",
    "\n",
    "7. Hyperparameter Tuning:\n",
    "Fine-tune the hyperparameters of each model to optimize performance.\n",
    "\n",
    "8. Interpretation:\n",
    "Analyze the importance of different features, including BERT embeddings and topic distributions, for each dichotomy to gain insights into how they contribute to personality type prediction.\n",
    "By building "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import scikeras\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, make_scorer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.constraints import MaxNorm\n",
    "from scikeras.wrappers import KerasClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "\n",
    "df_all_cleaned = pd.read_csv(r'C:\\Users\\bella\\Downloads\\Y3Q2 langai\\df_all_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auhtor_ID</th>\n",
       "      <th>post</th>\n",
       "      <th>extrovert</th>\n",
       "      <th>feeling</th>\n",
       "      <th>judging</th>\n",
       "      <th>sensing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_12bhu7</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_12jbpd</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_12uwr5</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_12zm15</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_13cjjl</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>t2_vfp8y</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>t2_w0842</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>t2_w6rgl</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>t2_wilcwvo</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t2_zq7gkv</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auhtor_ID                                               post  extrovert  \\\n",
       "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
       "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
       "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
       "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
       "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
       "..          ...                                                ...        ...   \n",
       "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
       "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
       "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
       "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
       "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
       "\n",
       "     feeling  judging  sensing  \n",
       "0        1.0      0.0      0.0  \n",
       "1        0.0      0.0      0.0  \n",
       "2        0.0      1.0      0.0  \n",
       "3        1.0      0.0      0.0  \n",
       "4        0.0      0.0      1.0  \n",
       "..       ...      ...      ...  \n",
       "150      0.0      1.0      0.0  \n",
       "151      0.0      1.0      0.0  \n",
       "152      0.0      1.0      0.0  \n",
       "153      0.0      0.0      0.0  \n",
       "154      1.0      1.0      0.0  \n",
       "\n",
       "[155 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop columns 'post_feeling', 'post_judging', 'post_sensing' and rename 'post_extrovert' to 'post'\n",
    "\n",
    "df_all_cleaned = df_all_cleaned.drop(['post_feeling', 'post_judging', 'post_sensing'], axis=1)\n",
    "df_all_cleaned = df_all_cleaned.rename(columns={'post_extrovert': 'post'})\n",
    "df_all_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\bella\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\bella\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# preprocess sentences\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auhtor_ID</th>\n",
       "      <th>post</th>\n",
       "      <th>extrovert</th>\n",
       "      <th>feeling</th>\n",
       "      <th>judging</th>\n",
       "      <th>sensing</th>\n",
       "      <th>processed_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_12bhu7</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wear lorna shore shirt alot public lewd long s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_12jbpd</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>id say accurate characterization ni users read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_12uwr5</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ya know like people home decorations could sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_12zm15</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>true tho theyre kinda interesting buuuut issue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_13cjjl</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yeah thats one things make better objectively ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>t2_vfp8y</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>change profession would inadmissible country p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>t2_w0842</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technological singularity possibility contribu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>t2_w6rgl</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dear god man chill im einstein hawking serious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>t2_wilcwvo</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thats fake lib would say human blood water url...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t2_zq7gkv</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>biggest problem asking dont need amount recipr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auhtor_ID                                               post  extrovert  \\\n",
       "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
       "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
       "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
       "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
       "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
       "..          ...                                                ...        ...   \n",
       "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
       "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
       "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
       "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
       "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
       "\n",
       "     feeling  judging  sensing  \\\n",
       "0        1.0      0.0      0.0   \n",
       "1        0.0      0.0      0.0   \n",
       "2        0.0      1.0      0.0   \n",
       "3        1.0      0.0      0.0   \n",
       "4        0.0      0.0      1.0   \n",
       "..       ...      ...      ...   \n",
       "150      0.0      1.0      0.0   \n",
       "151      0.0      1.0      0.0   \n",
       "152      0.0      1.0      0.0   \n",
       "153      0.0      0.0      0.0   \n",
       "154      1.0      1.0      0.0   \n",
       "\n",
       "                                        processed_post  \n",
       "0    wear lorna shore shirt alot public lewd long s...  \n",
       "1    id say accurate characterization ni users read...  \n",
       "2    ya know like people home decorations could sav...  \n",
       "3    true tho theyre kinda interesting buuuut issue...  \n",
       "4    yeah thats one things make better objectively ...  \n",
       "..                                                 ...  \n",
       "150  change profession would inadmissible country p...  \n",
       "151  technological singularity possibility contribu...  \n",
       "152  dear god man chill im einstein hawking serious...  \n",
       "153  thats fake lib would say human blood water url...  \n",
       "154  biggest problem asking dont need amount recipr...  \n",
       "\n",
       "[155 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply preprocessing \n",
    "\n",
    "df_all_cleaned['processed_post'] = df_all_cleaned['post'].apply(preprocess_text)\n",
    "df_all_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export this dataset, it will be used later.\n",
    "\n",
    "# df_all_cleaned.to_csv('df_all_cleaned_preprocessed.csv', sep=',', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      auhtor_ID                                               post  extrovert  \\\n",
      "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
      "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
      "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
      "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
      "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
      "..          ...                                                ...        ...   \n",
      "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
      "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
      "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
      "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
      "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
      "\n",
      "     feeling  judging  sensing  \\\n",
      "0        1.0      0.0      0.0   \n",
      "1        0.0      0.0      0.0   \n",
      "2        0.0      1.0      0.0   \n",
      "3        1.0      0.0      0.0   \n",
      "4        0.0      0.0      1.0   \n",
      "..       ...      ...      ...   \n",
      "150      0.0      1.0      0.0   \n",
      "151      0.0      1.0      0.0   \n",
      "152      0.0      1.0      0.0   \n",
      "153      0.0      0.0      0.0   \n",
      "154      1.0      1.0      0.0   \n",
      "\n",
      "                                        processed_post  \\\n",
      "0    wear lorna shore shirt alot public lewd long s...   \n",
      "1    id say accurate characterization ni users read...   \n",
      "2    ya know like people home decorations could sav...   \n",
      "3    true tho theyre kinda interesting buuuut issue...   \n",
      "4    yeah thats one things make better objectively ...   \n",
      "..                                                 ...   \n",
      "150  change profession would inadmissible country p...   \n",
      "151  technological singularity possibility contribu...   \n",
      "152  dear god man chill im einstein hawking serious...   \n",
      "153  thats fake lib would say human blood water url...   \n",
      "154  biggest problem asking dont need amount recipr...   \n",
      "\n",
      "                                       bert_embeddings  \n",
      "0    [0.03740044, 0.03744348, 0.40402788, -0.154586...  \n",
      "1    [-0.122634806, 0.06978733, 0.23516777, -0.1674...  \n",
      "2    [0.10358047, -0.079817355, 0.4862674, 0.006832...  \n",
      "3    [-0.11131706, 0.070213296, 0.5168624, 0.017204...  \n",
      "4    [0.21926472, 0.11031033, 0.28619415, 0.1073027...  \n",
      "..                                                 ...  \n",
      "150  [-0.17524596, 0.1841626, 0.44777465, -0.118973...  \n",
      "151  [-0.025105778, -0.08084041, 0.3463775, -0.0189...  \n",
      "152  [0.088415004, 0.22571912, 0.38455746, -0.03194...  \n",
      "153  [0.05957357, 0.10176626, 0.3839616, -0.0742901...  \n",
      "154  [-0.06220896, 0.07866823, 0.22912164, 0.002138...  \n",
      "\n",
      "[155 rows x 8 columns]\n"
     ]
    }
   ],
   "source": [
    "# getting BERT embeddings on preprocessed post.\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to obtain BERT embeddings for a text\n",
    "def get_bert_embeddings(text):\n",
    "    # Tokenize input text and convert to tensor\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "    # Get BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(tokens)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "\n",
    "    # Average the embeddings across tokens (you can modify this based on your needs)\n",
    "    avg_embedding = torch.mean(embeddings, dim=1).squeeze().numpy()\n",
    "\n",
    "    return avg_embedding\n",
    "\n",
    "df_all_cleaned['bert_embeddings'] = df_all_cleaned['post'].apply(get_bert_embeddings)\n",
    "print(df_all_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extrovert</th>\n",
       "      <th>feeling</th>\n",
       "      <th>judging</th>\n",
       "      <th>sensing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     extrovert  feeling  judging  sensing\n",
       "0          1.0      1.0      0.0      0.0\n",
       "1          1.0      0.0      0.0      0.0\n",
       "2          0.0      0.0      1.0      0.0\n",
       "3          0.0      1.0      0.0      0.0\n",
       "4          0.0      0.0      0.0      1.0\n",
       "..         ...      ...      ...      ...\n",
       "150        0.0      0.0      1.0      0.0\n",
       "151        0.0      0.0      1.0      0.0\n",
       "152        0.0      0.0      1.0      0.0\n",
       "153        1.0      0.0      0.0      0.0\n",
       "154        1.0      1.0      1.0      0.0\n",
       "\n",
       "[155 rows x 4 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a separate dataframe for y labels.\n",
    "\n",
    "df_labels =  pd.DataFrame()\n",
    "df_labels['extrovert']= df_all_cleaned['extrovert']\n",
    "df_labels['feeling']= df_all_cleaned['feeling']\n",
    "df_labels['judging']= df_all_cleaned['judging']\n",
    "df_labels['sensing']= df_all_cleaned['sensing']\n",
    "\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/10\n",
      "WARNING:tensorflow:From c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "4/4 [==============================] - 1s 63ms/step - loss: 0.6070 - accuracy: 0.3226 - val_loss: 0.5448 - val_accuracy: 0.3226\n",
      "Epoch 2/10\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.5431 - accuracy: 0.3710 - val_loss: 0.5458 - val_accuracy: 0.3226\n",
      "Epoch 3/10\n",
      "4/4 [==============================] - 0s 10ms/step - loss: 0.5420 - accuracy: 0.3710 - val_loss: 0.5576 - val_accuracy: 0.3226\n",
      "Epoch 4/10\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5248 - accuracy: 0.3710 - val_loss: 0.5395 - val_accuracy: 0.3226\n",
      "Epoch 5/10\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5160 - accuracy: 0.3710 - val_loss: 0.5338 - val_accuracy: 0.3226\n",
      "Epoch 6/10\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5120 - accuracy: 0.3710 - val_loss: 0.5436 - val_accuracy: 0.3226\n",
      "Epoch 7/10\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4979 - accuracy: 0.3710 - val_loss: 0.5421 - val_accuracy: 0.3548\n",
      "Epoch 8/10\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4872 - accuracy: 0.3871 - val_loss: 0.5314 - val_accuracy: 0.3548\n",
      "Epoch 9/10\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4799 - accuracy: 0.3710 - val_loss: 0.5445 - val_accuracy: 0.3226\n",
      "Epoch 10/10\n",
      "4/4 [==============================] - 0s 11ms/step - loss: 0.4702 - accuracy: 0.3871 - val_loss: 0.5447 - val_accuracy: 0.3548\n",
      "1/1 [==============================] - 0s 75ms/step\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# features (X) and labels (y)\n",
    "X = np.vstack(df_all_cleaned['bert_embeddings'])\n",
    "y = df_labels\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, df_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a simple neural network model\n",
    "model = Sequential([\n",
    "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(df_labels.shape[1], activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_test, y_test))\n",
    "\n",
    "# Predictions\n",
    "nn_predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for extrovert (Neural Network): 0.7741935483870968\n",
      "Classification Report for extrovert (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.92      0.87        25\n",
      "         1.0       0.33      0.17      0.22         6\n",
      "\n",
      "    accuracy                           0.77        31\n",
      "   macro avg       0.58      0.54      0.55        31\n",
      "weighted avg       0.73      0.77      0.74        31\n",
      "\n",
      "Accuracy for feeling (Neural Network): 0.7419354838709677\n",
      "Classification Report for feeling (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      1.00      0.85        23\n",
      "         1.0       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.74        31\n",
      "   macro avg       0.37      0.50      0.43        31\n",
      "weighted avg       0.55      0.74      0.63        31\n",
      "\n",
      "Accuracy for judging (Neural Network): 0.41935483870967744\n",
      "Classification Report for judging (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.25      0.06      0.10        16\n",
      "         1.0       0.44      0.80      0.57        15\n",
      "\n",
      "    accuracy                           0.42        31\n",
      "   macro avg       0.35      0.43      0.34        31\n",
      "weighted avg       0.34      0.42      0.33        31\n",
      "\n",
      "Accuracy for sensing (Neural Network): 0.8709677419354839\n",
      "Classification Report for sensing (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93        27\n",
      "         1.0       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.87        31\n",
      "   macro avg       0.44      0.50      0.47        31\n",
      "weighted avg       0.76      0.87      0.81        31\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bella\\AppData\\Local\\Temp\\ipykernel_14996\\1734260993.py:2: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, true_labels in df_labels.iteritems():\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate for each df_lables\n",
    "for column, true_labels in df_labels.iteritems():\n",
    "    i = df_labels.columns.get_loc(column)  # Get the index of the current column\n",
    "    threshold = 0.5  # Adjust the threshold based on your task\n",
    "    binary_predictions = (nn_predictions[:, i] > threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_test[column], binary_predictions)\n",
    "    print(f\"Accuracy for {column} (Neural Network): {accuracy}\")\n",
    "\n",
    "    # Classification report for each dichotomy\n",
    "    print(f\"Classification Report for {column} (Neural Network):\")\n",
    "    print(classification_report(y_test[column], binary_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.412118 using {'batch_size': 100, 'epochs': 100}\n",
      "0.355014 (0.025132) with: {'batch_size': 10, 'epochs': 10}\n",
      "0.387727 (0.056188) with: {'batch_size': 10, 'epochs': 50}\n",
      "0.371467 (0.044918) with: {'batch_size': 10, 'epochs': 100}\n",
      "0.371274 (0.048022) with: {'batch_size': 20, 'epochs': 10}\n",
      "0.396051 (0.078653) with: {'batch_size': 20, 'epochs': 50}\n",
      "0.355788 (0.083221) with: {'batch_size': 20, 'epochs': 100}\n",
      "0.363144 (0.036560) with: {'batch_size': 40, 'epochs': 10}\n",
      "0.363144 (0.023313) with: {'batch_size': 40, 'epochs': 50}\n",
      "0.387340 (0.023560) with: {'batch_size': 40, 'epochs': 100}\n",
      "0.371274 (0.048022) with: {'batch_size': 60, 'epochs': 10}\n",
      "0.331010 (0.043797) with: {'batch_size': 60, 'epochs': 50}\n",
      "0.379791 (0.067263) with: {'batch_size': 60, 'epochs': 100}\n",
      "0.371274 (0.048022) with: {'batch_size': 80, 'epochs': 10}\n",
      "0.322687 (0.031288) with: {'batch_size': 80, 'epochs': 50}\n",
      "0.363144 (0.036560) with: {'batch_size': 80, 'epochs': 100}\n",
      "0.371274 (0.048022) with: {'batch_size': 100, 'epochs': 10}\n",
      "0.362756 (0.016269) with: {'batch_size': 100, 'epochs': 50}\n",
      "0.412118 (0.072545) with: {'batch_size': 100, 'epochs': 100}\n"
     ]
    }
   ],
   "source": [
    "# batch size and epochs grid search\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(64, activation='sigmoid'))\n",
    "    model.add(Dense(df_labels.shape[1], activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100]\n",
    "param_grid = dict(batch_size=batch_size, epochs=epochs)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.388115 using {'optimizer': 'Nadam'}\n",
      "0.371274 (0.048022) with: {'optimizer': 'SGD'}\n",
      "0.371274 (0.065491) with: {'optimizer': 'RMSprop'}\n",
      "0.154278 (0.201581) with: {'optimizer': 'Adagrad'}\n",
      "0.079365 (0.112239) with: {'optimizer': 'Adadelta'}\n",
      "0.364111 (0.107798) with: {'optimizer': 'Adam'}\n",
      "0.355207 (0.033803) with: {'optimizer': 'Adamax'}\n",
      "0.388115 (0.089798) with: {'optimizer': 'Nadam'}\n"
     ]
    }
   ],
   "source": [
    "# optimizer grid search\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(64, activation='sigmoid'))\n",
    "    model.add(Dense(df_labels.shape[1], activation='sigmoid'))\n",
    "    # Compile model\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, loss=\"binary_crossentropy\", epochs= 100, batch_size =100, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "optimizer = ['SGD', 'RMSprop', 'Adagrad', 'Adadelta', 'Adam', 'Adamax', 'Nadam']\n",
    "param_grid = dict(optimizer=optimizer)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.388115 using {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.6}\n",
      "0.371274 (0.048022) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.0}\n",
      "0.371274 (0.048022) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.2}\n",
      "0.371274 (0.048022) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.4}\n",
      "0.371274 (0.048022) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.6}\n",
      "0.371274 (0.048022) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.8}\n",
      "0.371274 (0.048022) with: {'optimizer__learning_rate': 0.001, 'optimizer__momentum': 0.9}\n",
      "0.371274 (0.048022) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.0}\n",
      "0.371274 (0.048022) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.2}\n",
      "0.371274 (0.048022) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.4}\n",
      "0.371274 (0.048022) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.6}\n",
      "0.355014 (0.025132) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.8}\n",
      "0.338947 (0.023065) with: {'optimizer__learning_rate': 0.01, 'optimizer__momentum': 0.9}\n",
      "0.322106 (0.057600) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.0}\n",
      "0.330623 (0.069727) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.2}\n",
      "0.371467 (0.044918) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.4}\n",
      "0.388115 (0.094111) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.6}\n",
      "0.323268 (0.063433) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.8}\n",
      "0.379597 (0.063573) with: {'optimizer__learning_rate': 0.1, 'optimizer__momentum': 0.9}\n",
      "0.355401 (0.053148) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.0}\n",
      "0.339141 (0.054896) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.2}\n",
      "0.322687 (0.070320) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.4}\n",
      "0.355401 (0.053148) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.6}\n",
      "0.371854 (0.083287) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.8}\n",
      "0.347077 (0.033192) with: {'optimizer__learning_rate': 0.2, 'optimizer__momentum': 0.9}\n",
      "0.160279 (0.090904) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.0}\n",
      "0.273906 (0.027082) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.2}\n",
      "0.347464 (0.063693) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.4}\n",
      "0.274100 (0.021542) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.6}\n",
      "0.290360 (0.020184) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.8}\n",
      "0.315137 (0.088465) with: {'optimizer__learning_rate': 0.3, 'optimizer__momentum': 0.9}\n"
     ]
    }
   ],
   "source": [
    "# grid search on learning rate and momentum\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(64, activation='sigmoid'))\n",
    "    model.add(Dense(df_labels.shape[1], activation='sigmoid'))\n",
    "    # Compile model\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, loss=\"binary_crossentropy\", optimizer=\"SGD\", epochs=100, batch_size=10, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "momentum = [0.0, 0.2, 0.4, 0.6, 0.8, 0.9]\n",
    "param_grid = dict(optimizer__learning_rate=learn_rate, optimizer__momentum=momentum)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.403600 using {'model__init_mode': 'glorot_normal'}\n",
      "0.371854 (0.078381) with: {'model__init_mode': 'uniform'}\n",
      "0.395858 (0.061047) with: {'model__init_mode': 'lecun_uniform'}\n",
      "0.387727 (0.056188) with: {'model__init_mode': 'normal'}\n",
      "0.379597 (0.049549) with: {'model__init_mode': 'zero'}\n",
      "0.403600 (0.044401) with: {'model__init_mode': 'glorot_normal'}\n",
      "0.396051 (0.078653) with: {'model__init_mode': 'glorot_uniform'}\n",
      "0.387727 (0.056188) with: {'model__init_mode': 'he_normal'}\n",
      "0.363724 (0.074701) with: {'model__init_mode': 'he_uniform'}\n"
     ]
    }
   ],
   "source": [
    "# grid search on initial mode\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "def create_model(init_mode='uniform'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(64, activation='sigmoid'))\n",
    "    model.add(Dense(df_labels.shape[1], activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=60, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "init_mode = ['uniform', 'lecun_uniform', 'normal', 'zero', 'glorot_normal', 'glorot_uniform', 'he_normal', 'he_uniform']\n",
    "param_grid = dict(model__init_mode=init_mode)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.403794 using {'model__activation': 'softplus'}\n",
      "0.363144 (0.036560) with: {'model__activation': 'softmax'}\n",
      "0.403794 (0.049823) with: {'model__activation': 'softplus'}\n",
      "0.387534 (0.038326) with: {'model__activation': 'softsign'}\n",
      "0.388115 (0.089798) with: {'model__activation': 'relu'}\n",
      "0.355594 (0.066993) with: {'model__activation': 'tanh'}\n",
      "0.371661 (0.063953) with: {'model__activation': 'sigmoid'}\n",
      "0.371661 (0.063953) with: {'model__activation': 'hard_sigmoid'}\n",
      "0.387727 (0.056188) with: {'model__activation': 'linear'}\n"
     ]
    }
   ],
   "source": [
    "# grid search activation\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "def create_model(activation='relu'):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, kernel_initializer='he_uniform', activation=activation, input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dense(64, kernel_initializer='he_uniform', activation='sigmoid'))\n",
    "    model.add(Dense(df_labels.shape[1], activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=60, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "activation = ['softmax', 'softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear']\n",
    "param_grid = dict(model__activation=activation)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:700: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.420054 using {'model__dropout_rate': 0.3, 'model__weight_constraint': 1.0}\n",
      "0.395858 (0.064213) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 1.0}\n",
      "0.395858 (0.064213) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 2.0}\n",
      "0.387727 (0.056188) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 3.0}\n",
      "0.403794 (0.053656) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 4.0}\n",
      "0.395664 (0.045185) with: {'model__dropout_rate': 0.0, 'model__weight_constraint': 5.0}\n",
      "0.379985 (0.085837) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 1.0}\n",
      "0.396051 (0.078653) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 2.0}\n",
      "0.347658 (0.083032) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 3.0}\n",
      "0.355788 (0.083221) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 4.0}\n",
      "0.396051 (0.078653) with: {'model__dropout_rate': 0.1, 'model__weight_constraint': 5.0}\n",
      "0.379597 (0.049549) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 1.0}\n",
      "0.395664 (0.053244) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 2.0}\n",
      "0.371661 (0.063953) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 3.0}\n",
      "0.363918 (0.089526) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 4.0}\n",
      "0.412118 (0.072545) with: {'model__dropout_rate': 0.2, 'model__weight_constraint': 5.0}\n",
      "0.420054 (0.064474) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 1.0}\n",
      "0.387921 (0.074965) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 2.0}\n",
      "0.388115 (0.094111) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 3.0}\n",
      "0.371661 (0.063953) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 4.0}\n",
      "0.371661 (0.063953) with: {'model__dropout_rate': 0.3, 'model__weight_constraint': 5.0}\n",
      "0.396051 (0.078653) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 1.0}\n",
      "0.387727 (0.056188) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 2.0}\n",
      "0.387921 (0.072271) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 3.0}\n",
      "0.395858 (0.064213) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 4.0}\n",
      "0.387727 (0.056188) with: {'model__dropout_rate': 0.4, 'model__weight_constraint': 5.0}\n",
      "0.363531 (0.062610) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 1.0}\n",
      "0.387727 (0.056188) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 2.0}\n",
      "0.403988 (0.067534) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 3.0}\n",
      "0.355594 (0.066993) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 4.0}\n",
      "0.396051 (0.078653) with: {'model__dropout_rate': 0.5, 'model__weight_constraint': 5.0}\n",
      "0.371661 (0.063953) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 1.0}\n",
      "0.379791 (0.067263) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 2.0}\n",
      "0.363531 (0.062610) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 3.0}\n",
      "0.379985 (0.083495) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 4.0}\n",
      "0.379597 (0.053402) with: {'model__dropout_rate': 0.6, 'model__weight_constraint': 5.0}\n",
      "0.403988 (0.073171) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 1.0}\n",
      "0.387534 (0.043191) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 2.0}\n",
      "0.379404 (0.034065) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 3.0}\n",
      "0.379985 (0.083495) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 4.0}\n",
      "0.379791 (0.072921) with: {'model__dropout_rate': 0.7, 'model__weight_constraint': 5.0}\n",
      "0.331010 (0.033541) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 1.0}\n",
      "0.330623 (0.010140) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 2.0}\n",
      "0.346883 (0.013818) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 3.0}\n",
      "0.355014 (0.025132) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 4.0}\n",
      "0.314750 (0.053854) with: {'model__dropout_rate': 0.8, 'model__weight_constraint': 5.0}\n",
      "0.363144 (0.036560) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 1.0}\n",
      "0.355014 (0.025132) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 2.0}\n",
      "0.355014 (0.025132) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 3.0}\n",
      "0.355014 (0.025132) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 4.0}\n",
      "0.371274 (0.048022) with: {'model__dropout_rate': 0.9, 'model__weight_constraint': 5.0}\n"
     ]
    }
   ],
   "source": [
    "# grid search weight constraint, drop rate\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "def create_model(dropout_rate, weight_constraint):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, kernel_initializer='he_uniform', activation='relu', input_shape=(X_train.shape[1],)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(64, kernel_initializer='he_uniform', activation='sigmoid'))\n",
    "    model.add(Dense(df_labels.shape[1], activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=60, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "weight_constraint = [1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "dropout_rate = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "param_grid = dict(model__dropout_rate=dropout_rate, model__weight_constraint=weight_constraint)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best: 0.387534 using {'model__neurons': 25}\n",
      "0.363144 (0.054065) with: {'model__neurons': 1}\n",
      "0.371274 (0.048022) with: {'model__neurons': 5}\n",
      "0.371274 (0.048022) with: {'model__neurons': 10}\n",
      "0.355014 (0.025132) with: {'model__neurons': 15}\n",
      "0.330623 (0.049823) with: {'model__neurons': 20}\n",
      "0.387534 (0.043191) with: {'model__neurons': 25}\n",
      "0.371467 (0.044918) with: {'model__neurons': 30}\n"
     ]
    }
   ],
   "source": [
    "# grid search neurons\n",
    "\n",
    "# Assuming X_train, X_test, y_train, y_test are already defined\n",
    "\n",
    "def create_model(neurons):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(neurons, input_shape=(X_train.shape[1],), kernel_initializer='he_uniform', activation='relu', kernel_constraint=MaxNorm(1, axis=0)))\n",
    "    model.add(Dropout(0.0))\n",
    "    model.add(Dense(64, kernel_initializer='he_uniform', activation='sigmoid'))\n",
    "    model.add(Dense(df_labels.shape[1], activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# create model\n",
    "model = KerasClassifier(model=create_model, epochs=100, batch_size=60, verbose=0)\n",
    "\n",
    "# define the grid search parameters\n",
    "neurons = [1, 5, 10, 15, 20, 25, 30]\n",
    "param_grid = dict(model__neurons=neurons)\n",
    "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=3)\n",
    "grid_result = grid.fit(X_train, y_train, validation_data=(X_test, y_test))\n",
    "\n",
    "# summarize results\n",
    "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip(means, stds, params):\n",
    "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final model\n",
    "\n",
    "# X = np.vstack(df_all_cleaned['bert_embeddings'])\n",
    "# y = df_labels\n",
    "\n",
    "# # Train-test split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, df_labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build a simple neural network model\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=1, input_shape=(X_train.shape[1],), kernel_initializer='he_uniform', activation='relu', kernel_constraint=MaxNorm(1, axis=0)))\n",
    "    model.add(Dropout(0.0))\n",
    "    model.add(Dense(64, kernel_initializer='he_uniform', activation='sigmoid'))\n",
    "    model.add(Dense(df_labels.shape[1], activation='sigmoid'))\n",
    "    # Compile model\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "tf.random.set_seed(seed)\n",
    "\n",
    "# create model\n",
    "model1 = KerasClassifier(model=create_model, epochs=100, batch_size=100, verbose=0)\n",
    "\n",
    "# Train the model\n",
    "model1.fit(X_train, y_train, epochs=100, batch_size=100, validation_data=(X_test, y_test))\n",
    "# \n",
    "# Predictions\n",
    "predictions = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 0, 1, 0]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fine tuning parameters do not change the result of the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auhtor_ID</th>\n",
       "      <th>post</th>\n",
       "      <th>extrovert</th>\n",
       "      <th>feeling</th>\n",
       "      <th>judging</th>\n",
       "      <th>sensing</th>\n",
       "      <th>processed_post</th>\n",
       "      <th>bert_embeddings</th>\n",
       "      <th>personality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_12bhu7</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>wear lorna shore shirt alot public lewd long s...</td>\n",
       "      <td>[0.03740044, 0.03744348, 0.40402788, -0.154586...</td>\n",
       "      <td>1100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_12jbpd</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>id say accurate characterization ni users read...</td>\n",
       "      <td>[-0.122634806, 0.06978733, 0.23516777, -0.1674...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_12uwr5</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ya know like people home decorations could sav...</td>\n",
       "      <td>[0.10358047, -0.079817355, 0.4862674, 0.006832...</td>\n",
       "      <td>0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_12zm15</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>true tho theyre kinda interesting buuuut issue...</td>\n",
       "      <td>[-0.11131706, 0.070213296, 0.5168624, 0.017204...</td>\n",
       "      <td>0100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_13cjjl</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>yeah thats one things make better objectively ...</td>\n",
       "      <td>[0.21926472, 0.11031033, 0.28619415, 0.1073027...</td>\n",
       "      <td>0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>t2_vfp8y</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>change profession would inadmissible country p...</td>\n",
       "      <td>[-0.17524596, 0.1841626, 0.44777465, -0.118973...</td>\n",
       "      <td>0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>t2_w0842</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>technological singularity possibility contribu...</td>\n",
       "      <td>[-0.025105778, -0.08084041, 0.3463775, -0.0189...</td>\n",
       "      <td>0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>t2_w6rgl</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>dear god man chill im einstein hawking serious...</td>\n",
       "      <td>[0.088415004, 0.22571912, 0.38455746, -0.03194...</td>\n",
       "      <td>0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>t2_wilcwvo</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>thats fake lib would say human blood water url...</td>\n",
       "      <td>[0.05957357, 0.10176626, 0.3839616, -0.0742901...</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t2_zq7gkv</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>biggest problem asking dont need amount recipr...</td>\n",
       "      <td>[-0.06220896, 0.07866823, 0.22912164, 0.002138...</td>\n",
       "      <td>1110</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auhtor_ID                                               post  extrovert  \\\n",
       "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...          1   \n",
       "1     t2_12jbpd  I'd say this is a very accurate characterizati...          1   \n",
       "2     t2_12uwr5  Ya know like most people with home decorations...          0   \n",
       "3     t2_12zm15  It's true tho. They're kinda more interesting ...          0   \n",
       "4     t2_13cjjl  Yeah, but that's one of the things that make m...          0   \n",
       "..          ...                                                ...        ...   \n",
       "150    t2_vfp8y  so change profession then. this would be inadm...          0   \n",
       "151    t2_w0842  The technological singularity. And the possibi...          0   \n",
       "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...          0   \n",
       "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...          1   \n",
       "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...          1   \n",
       "\n",
       "     feeling  judging  sensing  \\\n",
       "0          1        0        0   \n",
       "1          0        0        0   \n",
       "2          0        1        0   \n",
       "3          1        0        0   \n",
       "4          0        0        1   \n",
       "..       ...      ...      ...   \n",
       "150        0        1        0   \n",
       "151        0        1        0   \n",
       "152        0        1        0   \n",
       "153        0        0        0   \n",
       "154        1        1        0   \n",
       "\n",
       "                                        processed_post  \\\n",
       "0    wear lorna shore shirt alot public lewd long s...   \n",
       "1    id say accurate characterization ni users read...   \n",
       "2    ya know like people home decorations could sav...   \n",
       "3    true tho theyre kinda interesting buuuut issue...   \n",
       "4    yeah thats one things make better objectively ...   \n",
       "..                                                 ...   \n",
       "150  change profession would inadmissible country p...   \n",
       "151  technological singularity possibility contribu...   \n",
       "152  dear god man chill im einstein hawking serious...   \n",
       "153  thats fake lib would say human blood water url...   \n",
       "154  biggest problem asking dont need amount recipr...   \n",
       "\n",
       "                                       bert_embeddings personality  \n",
       "0    [0.03740044, 0.03744348, 0.40402788, -0.154586...        1100  \n",
       "1    [-0.122634806, 0.06978733, 0.23516777, -0.1674...        1000  \n",
       "2    [0.10358047, -0.079817355, 0.4862674, 0.006832...        0010  \n",
       "3    [-0.11131706, 0.070213296, 0.5168624, 0.017204...        0100  \n",
       "4    [0.21926472, 0.11031033, 0.28619415, 0.1073027...        0001  \n",
       "..                                                 ...         ...  \n",
       "150  [-0.17524596, 0.1841626, 0.44777465, -0.118973...        0010  \n",
       "151  [-0.025105778, -0.08084041, 0.3463775, -0.0189...        0010  \n",
       "152  [0.088415004, 0.22571912, 0.38455746, -0.03194...        0010  \n",
       "153  [0.05957357, 0.10176626, 0.3839616, -0.0742901...        1000  \n",
       "154  [-0.06220896, 0.07866823, 0.22912164, 0.002138...        1110  \n",
       "\n",
       "[155 rows x 9 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create columns that concatanates the columns extrovert, feeling, judging, sensing (does not add them but concatanates them), columns are floats\n",
    "df_all_cleaned['extrovert'] = df_all_cleaned['extrovert'].astype(int)\n",
    "df_all_cleaned['feeling'] = df_all_cleaned['feeling'].astype(int)\n",
    "df_all_cleaned['judging'] = df_all_cleaned['judging'].astype(int)\n",
    "df_all_cleaned['sensing'] = df_all_cleaned['sensing'].astype(int)\n",
    "df_all_cleaned['personality'] = df_all_cleaned['extrovert'].astype(str) + df_all_cleaned['feeling'].astype(str) + df_all_cleaned['judging'].astype(str) + df_all_cleaned['sensing'].astype(str)\n",
    "df_all_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for Combined MBTI Prediction: 0.3225806451612903\n"
     ]
    }
   ],
   "source": [
    "# Combine predictions for each dichotomy\n",
    "combined_predictions = np.hstack([predictions[:, i].reshape(-1, 1) for i in range(predictions.shape[1])])\n",
    "\n",
    "# Binarize the combined predictions\n",
    "binary_combined_predictions = (combined_predictions > 0.5).astype(int)\n",
    "\n",
    "# Calculate accuracy for the combined predictions\n",
    "combined_accuracy = accuracy_score(y_test, binary_combined_predictions)\n",
    "print(f\"Accuracy for Combined MBTI Prediction: {combined_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for extrovert (Neural Network): 0.8064516129032258\n",
      "Classification Report for extrovert (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      1.00      0.89        25\n",
      "         1.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.81        31\n",
      "   macro avg       0.40      0.50      0.45        31\n",
      "weighted avg       0.65      0.81      0.72        31\n",
      "\n",
      "Accuracy for feeling (Neural Network): 0.7419354838709677\n",
      "Classification Report for feeling (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      1.00      0.85        23\n",
      "         1.0       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.74        31\n",
      "   macro avg       0.37      0.50      0.43        31\n",
      "weighted avg       0.55      0.74      0.63        31\n",
      "\n",
      "Accuracy for judging (Neural Network): 0.4838709677419355\n",
      "Classification Report for judging (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        16\n",
      "         1.0       0.48      1.00      0.65        15\n",
      "\n",
      "    accuracy                           0.48        31\n",
      "   macro avg       0.24      0.50      0.33        31\n",
      "weighted avg       0.23      0.48      0.32        31\n",
      "\n",
      "Accuracy for sensing (Neural Network): 0.8709677419354839\n",
      "Classification Report for sensing (Neural Network):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93        27\n",
      "         1.0       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.87        31\n",
      "   macro avg       0.44      0.50      0.47        31\n",
      "weighted avg       0.76      0.87      0.81        31\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bella\\AppData\\Local\\Temp\\ipykernel_14996\\1440492696.py:2: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, true_labels in df_labels.iteritems():\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\bella\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluate for each df_lables\n",
    "for column, true_labels in df_labels.iteritems():\n",
    "    i = df_labels.columns.get_loc(column)  # Get the index of the current column\n",
    "    threshold = 0.5  # Adjust the threshold based on your task\n",
    "    binary_predictions = (predictions[:, i] > threshold).astype(int)\n",
    "    accuracy = accuracy_score(y_test[column], binary_predictions)\n",
    "    print(f\"Accuracy for {column} (Neural Network): {accuracy}\")\n",
    "\n",
    "    # Classification report for each dichotomy\n",
    "    print(f\"Classification Report for {column} (Neural Network):\")\n",
    "    print(classification_report(y_test[column], binary_predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
