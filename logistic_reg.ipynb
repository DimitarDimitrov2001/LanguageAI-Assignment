{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import sklearn\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/personality.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\dimit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dimit\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    return processed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auhtor_ID</th>\n",
       "      <th>post_extrovert</th>\n",
       "      <th>extrovert</th>\n",
       "      <th>post_feeling</th>\n",
       "      <th>feeling</th>\n",
       "      <th>post_judging</th>\n",
       "      <th>judging</th>\n",
       "      <th>post_sensing</th>\n",
       "      <th>sensing</th>\n",
       "      <th>processed_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_12bhu7</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wear lorna shore shirt alot public lewd long s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_12jbpd</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>id say accurate characterization ni users read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_12uwr5</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ya know like people home decorations could sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_12zm15</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>true tho theyre kinda interesting buuuut issue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_13cjjl</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yeah thats one things make better objectively ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>t2_vfp8y</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>change profession would inadmissible country p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>t2_w0842</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technological singularity possibility contribu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>t2_w6rgl</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dear god man chill im einstein hawking serious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>t2_wilcwvo</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thats fake lib would say human blood water url...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t2_zq7gkv</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>biggest problem asking dont need amount recipr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auhtor_ID                                     post_extrovert  extrovert  \\\n",
       "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
       "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
       "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
       "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
       "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
       "..          ...                                                ...        ...   \n",
       "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
       "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
       "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
       "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
       "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
       "\n",
       "                                          post_feeling  feeling  \\\n",
       "0    I wear a Lorna shore shirt out alot in public ...      1.0   \n",
       "1    I'd say this is a very accurate characterizati...      0.0   \n",
       "2    Ya know like most people with home decorations...      0.0   \n",
       "3    It's true tho. They're kinda more interesting ...      1.0   \n",
       "4    Yeah, but that's one of the things that make m...      0.0   \n",
       "..                                                 ...      ...   \n",
       "150  so change profession then. this would be inadm...      0.0   \n",
       "151  The technological singularity. And the possibi...      0.0   \n",
       "152  Dear God man. Chill. I'm not Einstein or Hawki...      0.0   \n",
       "153  That's what a fake lib would say [Human blood ...      0.0   \n",
       "154  My biggest problem is asking for it. I don’t n...      1.0   \n",
       "\n",
       "                                          post_judging  judging  \\\n",
       "0    I wear a Lorna shore shirt out alot in public ...      0.0   \n",
       "1    I'd say this is a very accurate characterizati...      0.0   \n",
       "2    Ya know like most people with home decorations...      1.0   \n",
       "3    It's true tho. They're kinda more interesting ...      0.0   \n",
       "4    Yeah, but that's one of the things that make m...      0.0   \n",
       "..                                                 ...      ...   \n",
       "150  so change profession then. this would be inadm...      1.0   \n",
       "151  The technological singularity. And the possibi...      1.0   \n",
       "152  Dear God man. Chill. I'm not Einstein or Hawki...      1.0   \n",
       "153  That's what a fake lib would say [Human blood ...      0.0   \n",
       "154  My biggest problem is asking for it. I don’t n...      1.0   \n",
       "\n",
       "                                          post_sensing  sensing  \\\n",
       "0    I wear a Lorna shore shirt out alot in public ...      0.0   \n",
       "1    I'd say this is a very accurate characterizati...      0.0   \n",
       "2    Ya know like most people with home decorations...      0.0   \n",
       "3    It's true tho. They're kinda more interesting ...      0.0   \n",
       "4    Yeah, but that's one of the things that make m...      1.0   \n",
       "..                                                 ...      ...   \n",
       "150  so change profession then. this would be inadm...      0.0   \n",
       "151  The technological singularity. And the possibi...      0.0   \n",
       "152  Dear God man. Chill. I'm not Einstein or Hawki...      0.0   \n",
       "153  That's what a fake lib would say [Human blood ...      0.0   \n",
       "154  My biggest problem is asking for it. I don’t n...      0.0   \n",
       "\n",
       "                                        processed_post  \n",
       "0    wear lorna shore shirt alot public lewd long s...  \n",
       "1    id say accurate characterization ni users read...  \n",
       "2    ya know like people home decorations could sav...  \n",
       "3    true tho theyre kinda interesting buuuut issue...  \n",
       "4    yeah thats one things make better objectively ...  \n",
       "..                                                 ...  \n",
       "150  change profession would inadmissible country p...  \n",
       "151  technological singularity possibility contribu...  \n",
       "152  dear god man chill im einstein hawking serious...  \n",
       "153  thats fake lib would say human blood water url...  \n",
       "154  biggest problem asking dont need amount recipr...  \n",
       "\n",
       "[155 rows x 10 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['processed_post'] = data['post_extrovert'].apply(preprocess_text)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      auhtor_ID                                     post_extrovert  extrovert  \\\n",
      "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
      "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
      "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
      "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
      "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
      "..          ...                                                ...        ...   \n",
      "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
      "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
      "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
      "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
      "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
      "\n",
      "                                          post_feeling  feeling  \\\n",
      "0    I wear a Lorna shore shirt out alot in public ...      1.0   \n",
      "1    I'd say this is a very accurate characterizati...      0.0   \n",
      "2    Ya know like most people with home decorations...      0.0   \n",
      "3    It's true tho. They're kinda more interesting ...      1.0   \n",
      "4    Yeah, but that's one of the things that make m...      0.0   \n",
      "..                                                 ...      ...   \n",
      "150  so change profession then. this would be inadm...      0.0   \n",
      "151  The technological singularity. And the possibi...      0.0   \n",
      "152  Dear God man. Chill. I'm not Einstein or Hawki...      0.0   \n",
      "153  That's what a fake lib would say [Human blood ...      0.0   \n",
      "154  My biggest problem is asking for it. I don’t n...      1.0   \n",
      "\n",
      "                                          post_judging  judging  \\\n",
      "0    I wear a Lorna shore shirt out alot in public ...      0.0   \n",
      "1    I'd say this is a very accurate characterizati...      0.0   \n",
      "2    Ya know like most people with home decorations...      1.0   \n",
      "3    It's true tho. They're kinda more interesting ...      0.0   \n",
      "4    Yeah, but that's one of the things that make m...      0.0   \n",
      "..                                                 ...      ...   \n",
      "150  so change profession then. this would be inadm...      1.0   \n",
      "151  The technological singularity. And the possibi...      1.0   \n",
      "152  Dear God man. Chill. I'm not Einstein or Hawki...      1.0   \n",
      "153  That's what a fake lib would say [Human blood ...      0.0   \n",
      "154  My biggest problem is asking for it. I don’t n...      1.0   \n",
      "\n",
      "                                          post_sensing  sensing  \\\n",
      "0    I wear a Lorna shore shirt out alot in public ...      0.0   \n",
      "1    I'd say this is a very accurate characterizati...      0.0   \n",
      "2    Ya know like most people with home decorations...      0.0   \n",
      "3    It's true tho. They're kinda more interesting ...      0.0   \n",
      "4    Yeah, but that's one of the things that make m...      1.0   \n",
      "..                                                 ...      ...   \n",
      "150  so change profession then. this would be inadm...      0.0   \n",
      "151  The technological singularity. And the possibi...      0.0   \n",
      "152  Dear God man. Chill. I'm not Einstein or Hawki...      0.0   \n",
      "153  That's what a fake lib would say [Human blood ...      0.0   \n",
      "154  My biggest problem is asking for it. I don’t n...      0.0   \n",
      "\n",
      "                                        processed_post  \\\n",
      "0    wear lorna shore shirt alot public lewd long s...   \n",
      "1    id say accurate characterization ni users read...   \n",
      "2    ya know like people home decorations could sav...   \n",
      "3    true tho theyre kinda interesting buuuut issue...   \n",
      "4    yeah thats one things make better objectively ...   \n",
      "..                                                 ...   \n",
      "150  change profession would inadmissible country p...   \n",
      "151  technological singularity possibility contribu...   \n",
      "152  dear god man chill im einstein hawking serious...   \n",
      "153  thats fake lib would say human blood water url...   \n",
      "154  biggest problem asking dont need amount recipr...   \n",
      "\n",
      "                                       bert_embeddings  \n",
      "0    [0.037400726, 0.03744322, 0.40402812, -0.15458...  \n",
      "1    [-0.12263443, 0.06978707, 0.23516819, -0.16746...  \n",
      "2    [0.1035808, -0.07981775, 0.48626775, 0.0068327...  \n",
      "3    [-0.11131683, 0.070212886, 0.51686287, 0.01720...  \n",
      "4    [0.21926501, 0.11031015, 0.2861948, 0.10730274...  \n",
      "..                                                 ...  \n",
      "150  [-0.17524561, 0.18416233, 0.44777521, -0.11897...  \n",
      "151  [-0.025105802, -0.08084059, 0.3463778, -0.0189...  \n",
      "152  [0.088415354, 0.22571889, 0.38455784, -0.03194...  \n",
      "153  [0.059574068, 0.1017659, 0.38396204, -0.074290...  \n",
      "154  [-0.06220877, 0.0786678, 0.22912213, 0.0021381...  \n",
      "\n",
      "[155 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to obtain BERT embeddings for a text\n",
    "def get_bert_embeddings(text):\n",
    "    # Tokenize input text and convert to tensor\n",
    "    tokens = tokenizer.encode(text, add_special_tokens=True, return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "    # Get BERT embeddings\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(tokens)\n",
    "        embeddings = outputs.last_hidden_state\n",
    "\n",
    "    # Average the embeddings across tokens (you can modify this based on your needs)\n",
    "    avg_embedding = torch.mean(embeddings, dim=1).squeeze().numpy()\n",
    "\n",
    "    return avg_embedding\n",
    "\n",
    "# # Example DataFrame with a 'processed_post' column\n",
    "# data = {'processed_post': [\"enjoy hiking spending time nature\",\n",
    "#                             \"text data preprocessing crucial nlp tasks\",\n",
    "#                             \"stop words removal improves text analysis\"]}\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Apply BERT embeddings to the 'processed_post' column\n",
    "# df['bert_embeddings'] = df['processed_post'].apply(get_bert_embeddings)\n",
    "\n",
    "# # Display the DataFrame with processed text and BERT embeddings\n",
    "# print(df)\n",
    "\n",
    "data['bert_embeddings'] = data['post_extrovert'].apply(get_bert_embeddings)\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auhtor_ID</th>\n",
       "      <th>post_extrovert</th>\n",
       "      <th>extrovert</th>\n",
       "      <th>post_feeling</th>\n",
       "      <th>feeling</th>\n",
       "      <th>post_judging</th>\n",
       "      <th>judging</th>\n",
       "      <th>post_sensing</th>\n",
       "      <th>sensing</th>\n",
       "      <th>processed_post</th>\n",
       "      <th>bert_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_12bhu7</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wear lorna shore shirt alot public lewd long s...</td>\n",
       "      <td>[0.037400726, 0.03744322, 0.40402812, -0.15458...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_12jbpd</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>id say accurate characterization ni users read...</td>\n",
       "      <td>[-0.12263443, 0.06978707, 0.23516819, -0.16746...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_12uwr5</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ya know like people home decorations could sav...</td>\n",
       "      <td>[0.1035808, -0.07981775, 0.48626775, 0.0068327...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_12zm15</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>true tho theyre kinda interesting buuuut issue...</td>\n",
       "      <td>[-0.11131683, 0.070212886, 0.51686287, 0.01720...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_13cjjl</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yeah thats one things make better objectively ...</td>\n",
       "      <td>[0.21926501, 0.11031015, 0.2861948, 0.10730274...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>t2_vfp8y</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>change profession would inadmissible country p...</td>\n",
       "      <td>[-0.17524561, 0.18416233, 0.44777521, -0.11897...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>t2_w0842</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technological singularity possibility contribu...</td>\n",
       "      <td>[-0.025105802, -0.08084059, 0.3463778, -0.0189...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>t2_w6rgl</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dear god man chill im einstein hawking serious...</td>\n",
       "      <td>[0.088415354, 0.22571889, 0.38455784, -0.03194...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>t2_wilcwvo</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thats fake lib would say human blood water url...</td>\n",
       "      <td>[0.059574068, 0.1017659, 0.38396204, -0.074290...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t2_zq7gkv</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>biggest problem asking dont need amount recipr...</td>\n",
       "      <td>[-0.06220877, 0.0786678, 0.22912213, 0.0021381...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auhtor_ID                                     post_extrovert  extrovert  \\\n",
       "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
       "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
       "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
       "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
       "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
       "..          ...                                                ...        ...   \n",
       "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
       "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
       "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
       "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
       "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
       "\n",
       "                                          post_feeling  feeling  \\\n",
       "0    I wear a Lorna shore shirt out alot in public ...      1.0   \n",
       "1    I'd say this is a very accurate characterizati...      0.0   \n",
       "2    Ya know like most people with home decorations...      0.0   \n",
       "3    It's true tho. They're kinda more interesting ...      1.0   \n",
       "4    Yeah, but that's one of the things that make m...      0.0   \n",
       "..                                                 ...      ...   \n",
       "150  so change profession then. this would be inadm...      0.0   \n",
       "151  The technological singularity. And the possibi...      0.0   \n",
       "152  Dear God man. Chill. I'm not Einstein or Hawki...      0.0   \n",
       "153  That's what a fake lib would say [Human blood ...      0.0   \n",
       "154  My biggest problem is asking for it. I don’t n...      1.0   \n",
       "\n",
       "                                          post_judging  judging  \\\n",
       "0    I wear a Lorna shore shirt out alot in public ...      0.0   \n",
       "1    I'd say this is a very accurate characterizati...      0.0   \n",
       "2    Ya know like most people with home decorations...      1.0   \n",
       "3    It's true tho. They're kinda more interesting ...      0.0   \n",
       "4    Yeah, but that's one of the things that make m...      0.0   \n",
       "..                                                 ...      ...   \n",
       "150  so change profession then. this would be inadm...      1.0   \n",
       "151  The technological singularity. And the possibi...      1.0   \n",
       "152  Dear God man. Chill. I'm not Einstein or Hawki...      1.0   \n",
       "153  That's what a fake lib would say [Human blood ...      0.0   \n",
       "154  My biggest problem is asking for it. I don’t n...      1.0   \n",
       "\n",
       "                                          post_sensing  sensing  \\\n",
       "0    I wear a Lorna shore shirt out alot in public ...      0.0   \n",
       "1    I'd say this is a very accurate characterizati...      0.0   \n",
       "2    Ya know like most people with home decorations...      0.0   \n",
       "3    It's true tho. They're kinda more interesting ...      0.0   \n",
       "4    Yeah, but that's one of the things that make m...      1.0   \n",
       "..                                                 ...      ...   \n",
       "150  so change profession then. this would be inadm...      0.0   \n",
       "151  The technological singularity. And the possibi...      0.0   \n",
       "152  Dear God man. Chill. I'm not Einstein or Hawki...      0.0   \n",
       "153  That's what a fake lib would say [Human blood ...      0.0   \n",
       "154  My biggest problem is asking for it. I don’t n...      0.0   \n",
       "\n",
       "                                        processed_post  \\\n",
       "0    wear lorna shore shirt alot public lewd long s...   \n",
       "1    id say accurate characterization ni users read...   \n",
       "2    ya know like people home decorations could sav...   \n",
       "3    true tho theyre kinda interesting buuuut issue...   \n",
       "4    yeah thats one things make better objectively ...   \n",
       "..                                                 ...   \n",
       "150  change profession would inadmissible country p...   \n",
       "151  technological singularity possibility contribu...   \n",
       "152  dear god man chill im einstein hawking serious...   \n",
       "153  thats fake lib would say human blood water url...   \n",
       "154  biggest problem asking dont need amount recipr...   \n",
       "\n",
       "                                       bert_embeddings  \n",
       "0    [0.037400726, 0.03744322, 0.40402812, -0.15458...  \n",
       "1    [-0.12263443, 0.06978707, 0.23516819, -0.16746...  \n",
       "2    [0.1035808, -0.07981775, 0.48626775, 0.0068327...  \n",
       "3    [-0.11131683, 0.070212886, 0.51686287, 0.01720...  \n",
       "4    [0.21926501, 0.11031015, 0.2861948, 0.10730274...  \n",
       "..                                                 ...  \n",
       "150  [-0.17524561, 0.18416233, 0.44777521, -0.11897...  \n",
       "151  [-0.025105802, -0.08084059, 0.3463778, -0.0189...  \n",
       "152  [0.088415354, 0.22571889, 0.38455784, -0.03194...  \n",
       "153  [0.059574068, 0.1017659, 0.38396204, -0.074290...  \n",
       "154  [-0.06220877, 0.0786678, 0.22912213, 0.0021381...  \n",
       "\n",
       "[155 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['bert_embeddings'], data['extrovert'],test_size=0.2, random_state=42, stratify=data['extrovert'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_flattened = np.array([embedding.flatten() for embedding in X_train])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_flattened = np.array([embedding.flatten() for embedding in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(n_jobs=-1)\n",
    "lr.fit(X_train_flattened, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9032258064516129 0.7096774193548387\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.75      0.82        28\n",
      "         1.0       0.12      0.33      0.18         3\n",
      "\n",
      "    accuracy                           0.71        31\n",
      "   macro avg       0.52      0.54      0.50        31\n",
      "weighted avg       0.84      0.71      0.76        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(lr.predict(X_train_flattened), y_train),\n",
    "accuracy_score(lr.predict(X_test_flattened), y_test))\n",
    "# get classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(lr.predict(X_test_flattened), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['bert_embeddings'], data['feeling'],test_size=0.2, random_state=42, stratify=data['feeling'])\n",
    "X_train_flattened = np.array([embedding.flatten() for embedding in X_train])\n",
    "X_test_flattened = np.array([embedding.flatten() for embedding in X_test])\n",
    "\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "lr.fit(X_train_flattened, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9516129032258065 0.7419354838709677\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.91      0.77      0.83        26\n",
      "         1.0       0.33      0.60      0.43         5\n",
      "\n",
      "    accuracy                           0.74        31\n",
      "   macro avg       0.62      0.68      0.63        31\n",
      "weighted avg       0.82      0.74      0.77        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(lr.predict(X_train_flattened), y_train),\n",
    "accuracy_score(lr.predict(X_test_flattened), y_test))\n",
    "# get classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(lr.predict(X_test_flattened), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['bert_embeddings'], data['judging'],test_size=0.2, random_state=42, stratify=data['judging'])\n",
    "X_train_flattened = np.array([embedding.flatten() for embedding in X_train])\n",
    "X_test_flattened = np.array([embedding.flatten() for embedding in X_test])\n",
    "\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "lr.fit(X_train_flattened, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9758064516129032 0.5483870967741935\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.58      0.44      0.50        16\n",
      "         1.0       0.53      0.67      0.59        15\n",
      "\n",
      "    accuracy                           0.55        31\n",
      "   macro avg       0.55      0.55      0.54        31\n",
      "weighted avg       0.56      0.55      0.54        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(lr.predict(X_train_flattened), y_train),\n",
    "accuracy_score(lr.predict(X_test_flattened), y_test))\n",
    "# get classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(lr.predict(X_test_flattened), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(n_jobs=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(n_jobs=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(n_jobs=-1)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['bert_embeddings'], data['sensing'],test_size=0.2, random_state=42, stratify=data['sensing'])\n",
    "X_train_flattened = np.array([embedding.flatten() for embedding in X_train])\n",
    "X_test_flattened = np.array([embedding.flatten() for embedding in X_test])\n",
    "\n",
    "lr = LogisticRegression(n_jobs=-1)\n",
    "lr.fit(X_train_flattened, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9112903225806451 0.8387096774193549\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.96      0.87      0.91        30\n",
      "         1.0       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.84        31\n",
      "   macro avg       0.48      0.43      0.46        31\n",
      "weighted avg       0.93      0.84      0.88        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(lr.predict(X_train_flattened), y_train),\n",
    "accuracy_score(lr.predict(X_test_flattened), y_test))\n",
    "# get classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(lr.predict(X_test_flattened), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7420000000000001\n",
      "Best Hyperparameters: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89        25\n",
      "           1       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.81        31\n",
      "   macro avg       0.40      0.50      0.45        31\n",
      "weighted avg       0.65      0.81      0.72        31\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "225 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 66, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.68555556 0.68555556 0.68555556        nan        nan        nan\n",
      " 0.68555556 0.69088889 0.69355556        nan        nan        nan\n",
      " 0.73622222 0.73622222 0.74155556        nan        nan        nan\n",
      " 0.734      0.734      0.734             nan        nan        nan\n",
      " 0.742      0.742      0.742             nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# use grid search to find optimal parameters for logistic regression model for all 4 personality traits\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['bert_embeddings'], data['extrovert'],test_size=0.2, random_state=42)\n",
    "X_train_flattened = np.array([embedding.flatten() for embedding in X_train])\n",
    "X_test_flattened = np.array([embedding.flatten() for embedding in X_test])\n",
    "\n",
    "\n",
    "# define model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# define evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\n",
    "# define search space\n",
    "space = dict()\n",
    "space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space['penalty'] = ['l2', 'elasticnet']\n",
    "space['C'] = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "# define search\n",
    "search = GridSearchCV(model, space, scoring='accuracy', n_jobs=-1, cv=cv)\n",
    "\n",
    "# execute search\n",
    "result = search.fit(X_train_flattened, y_train)\n",
    "\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "\n",
    "# calculate classification report on test set\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, result.predict(X_test_flattened)))\n",
    "\n",
    "# save the predictions of the test set in dataframe with column name 'extrovert_pred'\n",
    "y_pred = result.predict(X_test_flattened)\n",
    "y_pred_df = pd.DataFrame(y_pred)\n",
    "y_pred_df.columns = ['extrovert_pred']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extrovert_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    extrovert_pred\n",
       "0                0\n",
       "1                0\n",
       "2                0\n",
       "3                0\n",
       "4                0\n",
       "5                0\n",
       "6                0\n",
       "7                0\n",
       "8                0\n",
       "9                0\n",
       "10               0\n",
       "11               0\n",
       "12               0\n",
       "13               0\n",
       "14               0\n",
       "15               0\n",
       "16               0\n",
       "17               0\n",
       "18               0\n",
       "19               0\n",
       "20               0\n",
       "21               0\n",
       "22               0\n",
       "23               0\n",
       "24               0\n",
       "25               0\n",
       "26               0\n",
       "27               0\n",
       "28               0\n",
       "29               0\n",
       "30               0"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "225 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 66, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.67266667 0.66988889 0.66988889        nan        nan        nan\n",
      " 0.68877778 0.68866667 0.68333333        nan        nan        nan\n",
      " 0.729      0.729      0.72366667        nan        nan        nan\n",
      " 0.691      0.691      0.69633333        nan        nan        nan\n",
      " 0.69366667 0.69366667 0.69366667        nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.7289999999999999\n",
      "Best Hyperparameters: {'C': 1.0, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      1.00      0.87        23\n",
      "           1       1.00      0.12      0.22         8\n",
      "\n",
      "    accuracy                           0.77        31\n",
      "   macro avg       0.88      0.56      0.55        31\n",
      "weighted avg       0.83      0.77      0.70        31\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# use grid search to find optimal parameters for logistic regression model for all 4 personality traits\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['bert_embeddings'], data['feeling'],test_size=0.2, random_state=42)\n",
    "X_train_flattened = np.array([embedding.flatten() for embedding in X_train])\n",
    "X_test_flattened = np.array([embedding.flatten() for embedding in X_test])\n",
    "\n",
    "\n",
    "# define model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# define evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\n",
    "# define search space\n",
    "space = dict()\n",
    "space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space['penalty'] = ['l2', 'elasticnet']\n",
    "space['C'] = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "# define search\n",
    "search = GridSearchCV(model, space, scoring='accuracy', n_jobs=-1, cv=cv)\n",
    "\n",
    "# execute search\n",
    "result = search.fit(X_train_flattened, y_train)\n",
    "\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "\n",
    "# calculate classification report on test set\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, result.predict(X_test_flattened)))\n",
    "\n",
    "# add predictions from the test to y_pred_df\n",
    "y_pred_df['feeling_pred'] = result.predict(X_test_flattened)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extrovert_pred</th>\n",
       "      <th>feeling_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    extrovert_pred  feeling_pred\n",
       "0                0             0\n",
       "1                0             0\n",
       "2                0             0\n",
       "3                0             0\n",
       "4                0             0\n",
       "5                0             0\n",
       "6                0             0\n",
       "7                0             0\n",
       "8                0             0\n",
       "9                0             0\n",
       "10               0             0\n",
       "11               0             0\n",
       "12               0             0\n",
       "13               0             0\n",
       "14               0             0\n",
       "15               0             0\n",
       "16               0             0\n",
       "17               0             0\n",
       "18               0             0\n",
       "19               0             0\n",
       "20               0             0\n",
       "21               0             0\n",
       "22               0             0\n",
       "23               0             0\n",
       "24               0             0\n",
       "25               0             0\n",
       "26               0             0\n",
       "27               0             0\n",
       "28               0             0\n",
       "29               0             0\n",
       "30               0             1"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     [-0.10599921, -0.11457217, 0.28883377, -0.0273...\n",
      "122    [-0.13501509, 0.14990366, 0.32966736, -0.04463...\n",
      "82     [-0.102551624, 0.121994734, 0.27854675, 0.0558...\n",
      "109    [-0.014521752, 0.12842986, 0.17662153, -0.0206...\n",
      "65     [0.24698451, 0.18807735, 0.52520406, -0.126965...\n",
      "                             ...                        \n",
      "71     [-0.0008798577, 0.05306895, 0.48110557, 0.0522...\n",
      "106    [-0.01522696, 0.08446585, 0.34010518, 0.011732...\n",
      "14     [0.13721104, 0.15530828, 0.31018516, -0.110464...\n",
      "92     [-0.12656039, 0.13525708, 0.33966416, -0.01012...\n",
      "102    [0.099014856, 0.15666161, 0.33694896, 0.048100...\n",
      "Name: bert_embeddings, Length: 124, dtype: object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.637\n",
      "Best Hyperparameters: {'C': 0.01, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        16\n",
      "           1       0.48      1.00      0.65        15\n",
      "\n",
      "    accuracy                           0.48        31\n",
      "   macro avg       0.24      0.50      0.33        31\n",
      "weighted avg       0.23      0.48      0.32        31\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "225 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 66, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.61844444 0.61311111 0.61044444        nan        nan        nan\n",
      " 0.62377778 0.62111111 0.61844444        nan        nan        nan\n",
      " 0.61055556 0.61055556 0.60777778        nan        nan        nan\n",
      " 0.61566667 0.61566667 0.621             nan        nan        nan\n",
      " 0.637      0.637      0.637             nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# use grid search to find optimal parameters for logistic regression model for all 4 personality traits\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['bert_embeddings'], data['judging'],test_size=0.2, random_state=42)\n",
    "X_train_flattened = np.array([embedding.flatten() for embedding in X_train])\n",
    "X_test_flattened = np.array([embedding.flatten() for embedding in X_test])\n",
    "\n",
    "print(X_train)\n",
    "\n",
    "# define model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# define evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\n",
    "# define search space\n",
    "space = dict()\n",
    "space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space['penalty'] = ['l2', 'elasticnet']\n",
    "space['C'] = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "# define search\n",
    "search = GridSearchCV(model, space, scoring='accuracy', n_jobs=-1, cv=cv)\n",
    "\n",
    "# execute search\n",
    "result = search.fit(X_train_flattened, y_train)\n",
    "\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "\n",
    "# calculate classification report on test set\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, result.predict(X_test_flattened)))\n",
    "\n",
    "# add predictions from the test to y_pred_df\n",
    "y_pred_df['judging_pred'] = result.predict(X_test_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     [-0.10599921, -0.11457217, 0.28883377, -0.0273...\n",
      "122    [-0.13501509, 0.14990366, 0.32966736, -0.04463...\n",
      "82     [-0.102551624, 0.121994734, 0.27854675, 0.0558...\n",
      "109    [-0.014521752, 0.12842986, 0.17662153, -0.0206...\n",
      "65     [0.24698451, 0.18807735, 0.52520406, -0.126965...\n",
      "                             ...                        \n",
      "71     [-0.0008798577, 0.05306895, 0.48110557, 0.0522...\n",
      "106    [-0.01522696, 0.08446585, 0.34010518, 0.011732...\n",
      "14     [0.13721104, 0.15530828, 0.31018516, -0.110464...\n",
      "92     [-0.12656039, 0.13525708, 0.33966416, -0.01012...\n",
      "102    [0.099014856, 0.15666161, 0.33694896, 0.048100...\n",
      "Name: bert_embeddings, Length: 124, dtype: object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.8873333333333335\n",
      "Best Hyperparameters: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      1.00      0.93        27\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.87        31\n",
      "   macro avg       0.44      0.50      0.47        31\n",
      "weighted avg       0.76      0.87      0.81        31\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "225 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 66, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.847      0.847      0.847             nan        nan        nan\n",
      " 0.855      0.855      0.85233333        nan        nan        nan\n",
      " 0.882      0.882      0.882             nan        nan        nan\n",
      " 0.88733333 0.88733333 0.88733333        nan        nan        nan\n",
      " 0.88733333 0.88733333 0.88733333        nan        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# use grid search to find optimal parameters for logistic regression model for all 4 personality traits\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['bert_embeddings'], data['sensing'],test_size=0.2, random_state=42)\n",
    "X_train_flattened = np.array([embedding.flatten() for embedding in X_train])\n",
    "X_test_flattened = np.array([embedding.flatten() for embedding in X_test])\n",
    "\n",
    "print(X_train)\n",
    "\n",
    "# define model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# define evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\n",
    "# define search space\n",
    "space = dict()\n",
    "space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space['penalty'] = ['l2', 'elasticnet']\n",
    "space['C'] = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "# define search\n",
    "search = GridSearchCV(model, space, scoring='accuracy', n_jobs=-1, cv=cv)\n",
    "\n",
    "# execute search\n",
    "result = search.fit(X_train_flattened, y_train)\n",
    "\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "\n",
    "# calculate classification report on test set\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, result.predict(X_test_flattened)))\n",
    "\n",
    "# add predictions from the test to y_pred_df\n",
    "y_pred_df['sensing_pred'] = result.predict(X_test_flattened)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extrovert_pred</th>\n",
       "      <th>feeling_pred</th>\n",
       "      <th>judging_pred</th>\n",
       "      <th>sensing_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    extrovert_pred  feeling_pred  judging_pred  sensing_pred\n",
       "0                0             0             1             0\n",
       "1                0             0             1             0\n",
       "2                0             0             1             0\n",
       "3                0             0             1             0\n",
       "4                0             0             1             0\n",
       "5                0             0             1             0\n",
       "6                0             0             1             0\n",
       "7                0             0             1             0\n",
       "8                0             0             1             0\n",
       "9                0             0             1             0\n",
       "10               0             0             1             0\n",
       "11               0             0             1             0\n",
       "12               0             0             1             0\n",
       "13               0             0             1             0\n",
       "14               0             0             1             0\n",
       "15               0             0             1             0\n",
       "16               0             0             1             0\n",
       "17               0             0             1             0\n",
       "18               0             0             1             0\n",
       "19               0             0             1             0\n",
       "20               0             0             1             0\n",
       "21               0             0             1             0\n",
       "22               0             0             1             0\n",
       "23               0             0             1             0\n",
       "24               0             0             1             0\n",
       "25               0             0             1             0\n",
       "26               0             0             1             0\n",
       "27               0             0             1             0\n",
       "28               0             0             1             0\n",
       "29               0             0             1             0\n",
       "30               0             1             1             0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrovert, feeling, judging, sensing transform to int\n",
    "y_pred_df['extrovert_pred'] = y_pred_df['extrovert_pred'].astype(int)\n",
    "y_pred_df['feeling_pred'] = y_pred_df['feeling_pred'].astype(int)\n",
    "y_pred_df['judging_pred'] = y_pred_df['judging_pred'].astype(int)\n",
    "y_pred_df['sensing_pred'] = y_pred_df['sensing_pred'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_df['personality'] = y_pred_df['extrovert_pred'].astype(str) + y_pred_df['feeling_pred'].astype(str) + y_pred_df['judging_pred'].astype(str) + y_pred_df['sensing_pred'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extrovert, feeling, judging, sensing transform to int\n",
    "data['extrovert'] = data['extrovert'].astype(int)\n",
    "data['feeling'] = data['feeling'].astype(int)\n",
    "data['judging'] = data['judging'].astype(int)\n",
    "data['sensing'] = data['sensing'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['personalty'] = data['extrovert'].astype(str) + data['feeling'].astype(str) + data['judging'].astype(str) + data['sensing'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81     0\n",
       "142    1\n",
       "31     0\n",
       "29     0\n",
       "118    0\n",
       "60     0\n",
       "93     0\n",
       "147    0\n",
       "153    0\n",
       "68     0\n",
       "42     0\n",
       "138    0\n",
       "78     0\n",
       "75     0\n",
       "15     1\n",
       "19     0\n",
       "30     0\n",
       "90     0\n",
       "117    0\n",
       "137    0\n",
       "18     0\n",
       "12     0\n",
       "9      0\n",
       "24     0\n",
       "69     0\n",
       "131    1\n",
       "95     0\n",
       "45     0\n",
       "86     0\n",
       "84     1\n",
       "126    0\n",
       "Name: sensing, dtype: int32"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "81     1000\n",
       "142    0001\n",
       "31     0110\n",
       "29     0000\n",
       "118    0110\n",
       "60     0010\n",
       "93     0010\n",
       "147    0010\n",
       "153    1000\n",
       "68     0010\n",
       "42     0010\n",
       "138    0110\n",
       "78     1000\n",
       "75     0000\n",
       "15     0001\n",
       "19     0010\n",
       "30     1000\n",
       "90     1000\n",
       "117    0110\n",
       "137    0010\n",
       "18     0100\n",
       "12     1000\n",
       "9      0010\n",
       "24     0100\n",
       "69     0010\n",
       "131    0001\n",
       "95     0000\n",
       "45     0110\n",
       "86     0010\n",
       "84     0001\n",
       "126    0100\n",
       "Name: personalty, dtype: object"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data['bert_embeddings'], data['personalty'],test_size=0.2, random_state=42)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0000       0.00      0.00      0.00         3\n",
      "        0001       0.00      0.00      0.00         4\n",
      "        0010       0.33      1.00      0.50        10\n",
      "        0100       0.00      0.00      0.00         3\n",
      "        0110       0.00      0.00      0.00         5\n",
      "        1000       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.32        31\n",
      "   macro avg       0.06      0.17      0.08        31\n",
      "weighted avg       0.11      0.32      0.16        31\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# calculate classification report on personality prediction\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred_df['personality']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96     [-0.10599921, -0.11457217, 0.28883377, -0.0273...\n",
      "122    [-0.13501509, 0.14990366, 0.32966736, -0.04463...\n",
      "82     [-0.102551624, 0.121994734, 0.27854675, 0.0558...\n",
      "109    [-0.014521752, 0.12842986, 0.17662153, -0.0206...\n",
      "65     [0.24698451, 0.18807735, 0.52520406, -0.126965...\n",
      "                             ...                        \n",
      "71     [-0.0008798577, 0.05306895, 0.48110557, 0.0522...\n",
      "106    [-0.01522696, 0.08446585, 0.34010518, 0.011732...\n",
      "14     [0.13721104, 0.15530828, 0.31018516, -0.110464...\n",
      "92     [-0.12656039, 0.13525708, 0.33966416, -0.01012...\n",
      "102    [0.099014856, 0.15666161, 0.33694896, 0.048100...\n",
      "Name: bert_embeddings, Length: 124, dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:725: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "225 fits failed out of a total of 450.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver newton-cg supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 56, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got elasticnet penalty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "75 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\base.py\", line 1151, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1168, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 66, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Only 'saga' solver supports elasticnet penalty, got solver=liblinear.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [0.36555556 0.35477778 0.36277778        nan        nan        nan\n",
      " 0.36544444 0.37088889 0.371             nan        nan        nan\n",
      " 0.36811111 0.36544444 0.36              nan        nan        nan\n",
      " 0.37888889 0.37888889 0.37077778        nan        nan        nan\n",
      " 0.371      0.371      0.371             nan        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Score: 0.37888888888888894\n",
      "Best Hyperparameters: {'C': 0.1, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        0000       0.00      0.00      0.00         3\n",
      "        0001       0.00      0.00      0.00         4\n",
      "        0010       0.31      0.80      0.44        10\n",
      "        0100       0.00      0.00      0.00         3\n",
      "        0110       0.00      0.00      0.00         5\n",
      "        1000       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.26        31\n",
      "   macro avg       0.05      0.13      0.07        31\n",
      "weighted avg       0.10      0.26      0.14        31\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\scipy\\optimize\\_linesearch.py:314: LineSearchWarning: The line search algorithm did not converge\n",
      "  warn('The line search algorithm did not converge', LineSearchWarning)\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\utils\\optimize.py:204: UserWarning: Line Search failed\n",
      "  warnings.warn(\"Line Search failed\")\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\dimit\\anaconda3\\envs\\DC3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# use grid search to find optimal parameters for logistic regression model for all 4 personality traits\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['bert_embeddings'], data['personalty'],test_size=0.2, random_state=42)\n",
    "X_train_flattened = np.array([embedding.flatten() for embedding in X_train])\n",
    "X_test_flattened = np.array([embedding.flatten() for embedding in X_test])\n",
    "\n",
    "print(X_train)\n",
    "\n",
    "# define model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# define evaluation\n",
    "cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)\n",
    "\n",
    "# define search space\n",
    "space = dict()\n",
    "space['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\n",
    "space['penalty'] = ['l2', 'elasticnet']\n",
    "space['C'] = [100, 10, 1.0, 0.1, 0.01]\n",
    "\n",
    "# define search\n",
    "search = GridSearchCV(model, space, scoring='accuracy', n_jobs=-1, cv=cv)\n",
    "\n",
    "# execute search\n",
    "result = search.fit(X_train_flattened, y_train)\n",
    "\n",
    "# summarize result\n",
    "print('Best Score: %s' % result.best_score_)\n",
    "print('Best Hyperparameters: %s' % result.best_params_)\n",
    "\n",
    "# calculate classification report on test set\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, result.predict(X_test_flattened)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DC3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
