{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import tensorflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_cleaned = pd.read_csv('data\\df_all_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auhtor_ID</th>\n",
       "      <th>post</th>\n",
       "      <th>extrovert</th>\n",
       "      <th>feeling</th>\n",
       "      <th>judging</th>\n",
       "      <th>sensing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_12bhu7</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_12jbpd</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_12uwr5</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_12zm15</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_13cjjl</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>t2_vfp8y</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>t2_w0842</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>t2_w6rgl</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>t2_wilcwvo</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t2_zq7gkv</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auhtor_ID                                               post  extrovert  \\\n",
       "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
       "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
       "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
       "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
       "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
       "..          ...                                                ...        ...   \n",
       "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
       "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
       "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
       "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
       "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
       "\n",
       "     feeling  judging  sensing  \n",
       "0        1.0      0.0      0.0  \n",
       "1        0.0      0.0      0.0  \n",
       "2        0.0      1.0      0.0  \n",
       "3        1.0      0.0      0.0  \n",
       "4        0.0      0.0      1.0  \n",
       "..       ...      ...      ...  \n",
       "150      0.0      1.0      0.0  \n",
       "151      0.0      1.0      0.0  \n",
       "152      0.0      1.0      0.0  \n",
       "153      0.0      0.0      0.0  \n",
       "154      1.0      1.0      0.0  \n",
       "\n",
       "[155 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_cleaned = df_all_cleaned.drop(['post_feeling', 'post_judging', 'post_sensing'], axis=1)\n",
    "df_all_cleaned = df_all_cleaned.rename(columns={'post_extrovert': 'post'})\n",
    "df_all_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\20212397\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\20212397\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters, numbers, and punctuation\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    processed_text = ' '.join(tokens)\n",
    "\n",
    "    return processed_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auhtor_ID</th>\n",
       "      <th>post</th>\n",
       "      <th>extrovert</th>\n",
       "      <th>feeling</th>\n",
       "      <th>judging</th>\n",
       "      <th>sensing</th>\n",
       "      <th>processed_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_12bhu7</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wear lorna shore shirt alot public lewd long s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_12jbpd</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>id say accurate characterization ni users read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_12uwr5</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ya know like people home decorations could sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_12zm15</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>true tho theyre kinda interesting buuuut issue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_13cjjl</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yeah thats one things make better objectively ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>t2_vfp8y</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>change profession would inadmissible country p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>t2_w0842</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technological singularity possibility contribu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>t2_w6rgl</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dear god man chill im einstein hawking serious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>t2_wilcwvo</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thats fake lib would say human blood water url...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t2_zq7gkv</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>biggest problem asking dont need amount recipr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auhtor_ID                                               post  extrovert  \\\n",
       "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
       "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
       "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
       "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
       "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
       "..          ...                                                ...        ...   \n",
       "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
       "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
       "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
       "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
       "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
       "\n",
       "     feeling  judging  sensing  \\\n",
       "0        1.0      0.0      0.0   \n",
       "1        0.0      0.0      0.0   \n",
       "2        0.0      1.0      0.0   \n",
       "3        1.0      0.0      0.0   \n",
       "4        0.0      0.0      1.0   \n",
       "..       ...      ...      ...   \n",
       "150      0.0      1.0      0.0   \n",
       "151      0.0      1.0      0.0   \n",
       "152      0.0      1.0      0.0   \n",
       "153      0.0      0.0      0.0   \n",
       "154      1.0      1.0      0.0   \n",
       "\n",
       "                                        processed_post  \n",
       "0    wear lorna shore shirt alot public lewd long s...  \n",
       "1    id say accurate characterization ni users read...  \n",
       "2    ya know like people home decorations could sav...  \n",
       "3    true tho theyre kinda interesting buuuut issue...  \n",
       "4    yeah thats one things make better objectively ...  \n",
       "..                                                 ...  \n",
       "150  change profession would inadmissible country p...  \n",
       "151  technological singularity possibility contribu...  \n",
       "152  dear god man chill im einstein hawking serious...  \n",
       "153  thats fake lib would say human blood water url...  \n",
       "154  biggest problem asking dont need amount recipr...  \n",
       "\n",
       "[155 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_cleaned['processed_post'] = df_all_cleaned['post'].apply(preprocess_text)\n",
    "df_all_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification using SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>auhtor_ID</th>\n",
       "      <th>post</th>\n",
       "      <th>extrovert</th>\n",
       "      <th>feeling</th>\n",
       "      <th>judging</th>\n",
       "      <th>sensing</th>\n",
       "      <th>processed_post</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>t2_12bhu7</td>\n",
       "      <td>I wear a Lorna shore shirt out alot in public ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>wear lorna shore shirt alot public lewd long s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>t2_12jbpd</td>\n",
       "      <td>I'd say this is a very accurate characterizati...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>id say accurate characterization ni users read...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>t2_12uwr5</td>\n",
       "      <td>Ya know like most people with home decorations...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>ya know like people home decorations could sav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>t2_12zm15</td>\n",
       "      <td>It's true tho. They're kinda more interesting ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>true tho theyre kinda interesting buuuut issue...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>t2_13cjjl</td>\n",
       "      <td>Yeah, but that's one of the things that make m...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>yeah thats one things make better objectively ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>t2_vfp8y</td>\n",
       "      <td>so change profession then. this would be inadm...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>change profession would inadmissible country p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>t2_w0842</td>\n",
       "      <td>The technological singularity. And the possibi...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>technological singularity possibility contribu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>t2_w6rgl</td>\n",
       "      <td>Dear God man. Chill. I'm not Einstein or Hawki...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dear god man chill im einstein hawking serious...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>t2_wilcwvo</td>\n",
       "      <td>That's what a fake lib would say [Human blood ...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>thats fake lib would say human blood water url...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>t2_zq7gkv</td>\n",
       "      <td>My biggest problem is asking for it. I don’t n...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>biggest problem asking dont need amount recipr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      auhtor_ID                                               post  extrovert  \\\n",
       "0     t2_12bhu7  I wear a Lorna shore shirt out alot in public ...        1.0   \n",
       "1     t2_12jbpd  I'd say this is a very accurate characterizati...        1.0   \n",
       "2     t2_12uwr5  Ya know like most people with home decorations...        0.0   \n",
       "3     t2_12zm15  It's true tho. They're kinda more interesting ...        0.0   \n",
       "4     t2_13cjjl  Yeah, but that's one of the things that make m...        0.0   \n",
       "..          ...                                                ...        ...   \n",
       "150    t2_vfp8y  so change profession then. this would be inadm...        0.0   \n",
       "151    t2_w0842  The technological singularity. And the possibi...        0.0   \n",
       "152    t2_w6rgl  Dear God man. Chill. I'm not Einstein or Hawki...        0.0   \n",
       "153  t2_wilcwvo  That's what a fake lib would say [Human blood ...        1.0   \n",
       "154   t2_zq7gkv  My biggest problem is asking for it. I don’t n...        1.0   \n",
       "\n",
       "     feeling  judging  sensing  \\\n",
       "0        1.0      0.0      0.0   \n",
       "1        0.0      0.0      0.0   \n",
       "2        0.0      1.0      0.0   \n",
       "3        1.0      0.0      0.0   \n",
       "4        0.0      0.0      1.0   \n",
       "..       ...      ...      ...   \n",
       "150      0.0      1.0      0.0   \n",
       "151      0.0      1.0      0.0   \n",
       "152      0.0      1.0      0.0   \n",
       "153      0.0      0.0      0.0   \n",
       "154      1.0      1.0      0.0   \n",
       "\n",
       "                                        processed_post  \n",
       "0    wear lorna shore shirt alot public lewd long s...  \n",
       "1    id say accurate characterization ni users read...  \n",
       "2    ya know like people home decorations could sav...  \n",
       "3    true tho theyre kinda interesting buuuut issue...  \n",
       "4    yeah thats one things make better objectively ...  \n",
       "..                                                 ...  \n",
       "150  change profession would inadmissible country p...  \n",
       "151  technological singularity possibility contribu...  \n",
       "152  dear god man chill im einstein hawking serious...  \n",
       "153  thats fake lib would say human blood water url...  \n",
       "154  biggest problem asking dont need amount recipr...  \n",
       "\n",
       "[155 rows x 7 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 7.98 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extrovert</th>\n",
       "      <th>feeling</th>\n",
       "      <th>judging</th>\n",
       "      <th>sensing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>155 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     extrovert  feeling  judging  sensing\n",
       "0          1.0      1.0      0.0      0.0\n",
       "1          1.0      0.0      0.0      0.0\n",
       "2          0.0      0.0      1.0      0.0\n",
       "3          0.0      1.0      0.0      0.0\n",
       "4          0.0      0.0      0.0      1.0\n",
       "..         ...      ...      ...      ...\n",
       "150        0.0      0.0      1.0      0.0\n",
       "151        0.0      0.0      1.0      0.0\n",
       "152        0.0      0.0      1.0      0.0\n",
       "153        1.0      0.0      0.0      0.0\n",
       "154        1.0      1.0      1.0      0.0\n",
       "\n",
       "[155 rows x 4 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "df_labels =  pd.DataFrame()\n",
    "df_labels['extrovert']= df_all_cleaned['extrovert']\n",
    "df_labels['feeling']= df_all_cleaned['feeling']\n",
    "df_labels['judging']= df_all_cleaned['judging']\n",
    "df_labels['sensing']= df_all_cleaned['sensing']\n",
    "\n",
    "df_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting dataset into training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_all_cleaned['processed_post'], df_labels, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8064516129032258\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.81      1.00      0.89        25\n",
      "         1.0       0.00      0.00      0.00         6\n",
      "\n",
      "    accuracy                           0.81        31\n",
      "   macro avg       0.40      0.50      0.45        31\n",
      "weighted avg       0.65      0.81      0.72        31\n",
      "\n",
      "[[25  0]\n",
      " [ 6  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# use SVM to classify the data\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# create a pipeline to vectorize the data and then classify it\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', SVC(kernel='linear', probability=True))])\n",
    "\n",
    "# fit the pipeline with the training data\n",
    "text_clf.fit(X_train, y_train['extrovert'])\n",
    "\n",
    "# predict the labels on the test data\n",
    "predictions = text_clf.predict(X_test)\n",
    "\n",
    "# print the accuracy score\n",
    "print(accuracy_score(y_test['extrovert'], predictions))\n",
    "\n",
    "# print the classification report\n",
    "print(classification_report(y_test['extrovert'], predictions))\n",
    "\n",
    "# print the confusion matrix\n",
    "print(confusion_matrix(y_test['extrovert'], predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7419354838709677\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      1.00      0.85        23\n",
      "         1.0       0.00      0.00      0.00         8\n",
      "\n",
      "    accuracy                           0.74        31\n",
      "   macro avg       0.37      0.50      0.43        31\n",
      "weighted avg       0.55      0.74      0.63        31\n",
      "\n",
      "[[23  0]\n",
      " [ 8  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4838709677419355\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.00      0.00      0.00        16\n",
      "         1.0       0.48      1.00      0.65        15\n",
      "\n",
      "    accuracy                           0.48        31\n",
      "   macro avg       0.24      0.50      0.33        31\n",
      "weighted avg       0.23      0.48      0.32        31\n",
      "\n",
      "[[ 0 16]\n",
      " [ 0 15]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8709677419354839\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.87      1.00      0.93        27\n",
      "         1.0       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.87        31\n",
      "   macro avg       0.44      0.50      0.47        31\n",
      "weighted avg       0.76      0.87      0.81        31\n",
      "\n",
      "[[27  0]\n",
      " [ 4  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# do the same for the other three labels\n",
    "text_clf.fit(X_train, y_train['feeling'])\n",
    "predictions = text_clf.predict(X_test)\n",
    "print(accuracy_score(y_test['feeling'], predictions))\n",
    "print(classification_report(y_test['feeling'], predictions))\n",
    "print(confusion_matrix(y_test['feeling'], predictions))\n",
    "\n",
    "text_clf.fit(X_train, y_train['judging'])\n",
    "predictions = text_clf.predict(X_test)\n",
    "print(accuracy_score(y_test['judging'], predictions))\n",
    "print(classification_report(y_test['judging'], predictions))\n",
    "print(confusion_matrix(y_test['judging'], predictions))\n",
    "\n",
    "text_clf.fit(X_train, y_train['sensing'])\n",
    "predictions = text_clf.predict(X_test)\n",
    "print(accuracy_score(y_test['sensing'], predictions))\n",
    "print(classification_report(y_test['sensing'], predictions))\n",
    "print(confusion_matrix(y_test['sensing'], predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "48 fits failed out of a total of 540.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "48 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 423, in fit\n",
      "    Xt = self._fit(X, y, **fit_params_steps)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 377, in _fit\n",
      "    X, fitted_transformer = fit_transform_one_cached(\n",
      "                            ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\joblib\\memory.py\", line 353, in __call__\n",
      "    return self.func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\pipeline.py\", line 957, in _fit_transform_one\n",
      "    res = transformer.fit_transform(X, y, **fit_params)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 2139, in fit_transform\n",
      "    X = super().fit_transform(raw_documents)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\base.py\", line 1152, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1402, in fit_transform\n",
      "    X, self.stop_words_ = self._limit_features(\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py\", line 1254, in _limit_features\n",
      "    raise ValueError(\n",
      "ValueError: After pruning, no terms remain. Try a lower min_df or a higher max_df.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\20212397\\OneDrive - TU Eindhoven\\Desktop\\Y3\\Y3Q2\\LangAI\\ASS\\LanguageAI-Assignment\\.venv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [0.742      0.742      0.742      0.742      0.742      0.742\n",
      "        nan        nan        nan 0.742      0.742      0.742\n",
      " 0.742      0.742      0.742      0.742      0.742      0.742\n",
      " 0.742      0.742      0.742      0.742      0.742      0.742\n",
      " 0.742      0.742      0.742      0.74233333 0.726      0.726\n",
      " 0.758      0.758      0.758             nan        nan        nan\n",
      " 0.726      0.726      0.726      0.774      0.76633333 0.76633333\n",
      " 0.766      0.766      0.766      0.734      0.734      0.734\n",
      " 0.734      0.734      0.734      0.742      0.742      0.742\n",
      " 0.742      0.74233333 0.72566667 0.75833333 0.76633333 0.76633333\n",
      "        nan        nan        nan 0.76633333 0.74233333 0.72566667\n",
      " 0.78233333 0.77466667 0.77466667 0.69333333 0.71733333 0.71733333\n",
      " 0.758      0.734      0.734      0.734      0.73366667 0.73366667\n",
      " 0.701      0.701      0.701      0.742      0.74233333 0.72566667\n",
      " 0.75833333 0.76633333 0.76633333        nan        nan        nan\n",
      " 0.76633333 0.74233333 0.72566667 0.78233333 0.77466667 0.77466667\n",
      " 0.69333333 0.71733333 0.71733333 0.758      0.734      0.734\n",
      " 0.71733333 0.72533333 0.72533333 0.693      0.70933333 0.70933333]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer(max_df=0.75, min_df=0.25)),\n",
       "                (&#x27;clf&#x27;, SVC(C=10.0, kernel=&#x27;linear&#x27;, probability=True))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;, TfidfVectorizer(max_df=0.75, min_df=0.25)),\n",
       "                (&#x27;clf&#x27;, SVC(C=10.0, kernel=&#x27;linear&#x27;, probability=True))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(max_df=0.75, min_df=0.25)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=10.0, kernel=&#x27;linear&#x27;, probability=True)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf', TfidfVectorizer(max_df=0.75, min_df=0.25)),\n",
       "                ('clf', SVC(C=10.0, kernel='linear', probability=True))])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use grid search to find the best parameters for the SVM classifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# create a pipeline to vectorize the data and then classify it\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer()),\n",
    "                     ('clf', SVC(kernel='linear', probability=True))])\n",
    "\n",
    "# set the parameters to be searched\n",
    "parameters = {'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "              'tfidf__max_df': [0.5, 0.75, 1.0],\n",
    "              'tfidf__min_df': [0.0, 0.25, 0.5],\n",
    "              'clf__C': [0.1, 1.0, 10.0, 100.0]}\n",
    "\n",
    "# create a grid search object\n",
    "grid_search = GridSearchCV(text_clf, parameters, n_jobs=-1, cv=5, verbose=1)\n",
    "\n",
    "# fit the grid search object to the training data\n",
    "grid_search.fit(X_train, y_train['extrovert'])\n",
    "\n",
    "# print the best parameters\n",
    "grid_search.best_params_\n",
    "\n",
    "# print the best score\n",
    "grid_search.best_score_\n",
    "\n",
    "# print the best estimator\n",
    "grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8064516129032258\n"
     ]
    }
   ],
   "source": [
    "#  use the best estimator to predict the labels on the test data\n",
    "predictions = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# print the accuracy score\n",
    "print(accuracy_score(y_test['extrovert'], predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
